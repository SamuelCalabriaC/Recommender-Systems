{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelCalabriaC/Recommender-Systems/blob/main/SR-GNN/SR_GNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijZojEg0uoFJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import math\n",
        "import warnings\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import pickle\n",
        "import argparse\n",
        "import datetime\n",
        "\n",
        "\"\"\"\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataset', default='sample', help='dataset name: diginetica/yoochoose1_4/yoochoose1_64/sample')\n",
        "parser.add_argument('--method', type=str, default='ggnn', help='ggnn/gat/gcn')\n",
        "parser.add_argument('--validation', action='store_true', help='validation')\n",
        "parser.add_argument('--epoch', type=int, default=30, help='number of epochs to train for')\n",
        "parser.add_argument('--batchSize', type=int, default=100, help='input batch size')\n",
        "parser.add_argument('--hiddenSize', type=int, default=100, help='hidden state size')\n",
        "parser.add_argument('--l2', type=float, default=1e-5, help='l2 penalty')\n",
        "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
        "parser.add_argument('--step', type=int, default=1, help='gnn propogation steps')\n",
        "parser.add_argument('--nonhybrid', action='store_true', help='global preference')\n",
        "parser.add_argument('--lr_dc', type=float, default=0.1, help='learning rate decay rate')\n",
        "parser.add_argument('--lr_dc_step', type=int, default=3, help='the number of steps after which the learning rate decay')\n",
        "opt = parser.parse_args()\n",
        "\"\"\"\n",
        "tf.disable_v2_behavior()\n",
        "tf.reset_default_graph()\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "train_data = pickle.load(open('train.txt', 'rb'))\n",
        "test_data = pickle.load(open('test.txt', 'rb'))\n",
        "# all_train_seq = pickle.load(open('../datasets/' + opt.dataset + '/all_train_seq.txt', 'rb'))\n",
        "\n",
        "\"\"\"if opt.dataset == 'diginetica':\n",
        "    n_node = 43098\n",
        "elif opt.dataset == 'yoochoose1_64' or opt.dataset == 'yoochoose1_4':\n",
        "    n_node = 37484\n",
        "else:\n",
        "  Tmall =  40727\n",
        "    n_node = 310\"\"\"\n",
        "\n",
        "# g = build_graph(all_train_seq)\n",
        "train_data = Data(train_data, sub_graph=True, method='ggnn', shuffle=True)\n",
        "test_data = Data(test_data, sub_graph=True, method='ggnn', shuffle=False)\n",
        "model = GGNN(hidden_size=100, out_size=100, batch_size=100, n_node= 48727,\n",
        "                 lr=0.001, l2=1e-5,  step=1, decay=3 * len(train_data.inputs) / 100, lr_dc=0.1,\n",
        "                 nonhybrid='store_true')\n",
        "print('CREACION DEL MODELO')\n",
        "best_result = [0, 0,0,0,0,0,0,0]\n",
        "best_epoch = [0, 0,0,0,0,0,0,0]\n",
        "for epoch in range(30):\n",
        "    print('epoch: ', epoch, '===========================================')\n",
        "    slices = train_data.generate_batch(model.batch_size)\n",
        "    fetches = [model.opt, model.loss_train, model.global_step]\n",
        "    print('start training: ', datetime.datetime.now())\n",
        "    loss_ = []\n",
        "    for i, j in zip(slices, np.arange(len(slices))):\n",
        "        adj_in, adj_out, alias, item, mask, targets = train_data.get_slice(i)\n",
        "        _, loss, _ = model.run(fetches, targets, item, adj_in, adj_out, alias,  mask)\n",
        "        loss_.append(loss)\n",
        "    loss = np.mean(loss_)\n",
        "    slices = test_data.generate_batch(model.batch_size)\n",
        "    print('start predicting: ', datetime.datetime.now())\n",
        "\n",
        "    hit20, mrr20, hit10, mrr10, hit5, mrr5, test_loss_ = [],[],[],[],[],[],[]\n",
        "    for i, j in zip(slices, np.arange(len(slices))):\n",
        "        adj_in, adj_out, alias, item, mask, targets = test_data.get_slice(i)\n",
        "        scores, test_loss = model.run([model.score_test, model.loss_test], targets, item, adj_in, adj_out, alias,  mask)\n",
        "        test_loss_.append(test_loss)\n",
        "        # Ordena las puntuaciones y devuelve los índices \n",
        "        index20 = np.argsort(scores, 1)[:, -20:]\n",
        "        index10 = np.argsort(scores, 1)[:,-10:]\n",
        "        index5 = np.argsort(scores, 1)[:,-5:]\n",
        "\n",
        "        cont = 0\n",
        "        for score, target in zip(index20, targets):\n",
        "            #print('Score: '+str(score)+' Target: '+str(target))\n",
        "            hit20.append(np.isin(target - 1, score))\n",
        "            if len(np.where(score == target - 1)[0]) == 0:\n",
        "                mrr20.append(0)\n",
        "            else:\n",
        "                mrr20.append(1 / (20-np.where(score == target - 1)[0][0]))\n",
        "\n",
        "        for score, target in zip(index10, targets):\n",
        "            hit10.append(np.isin(target - 1, score))\n",
        "            if len(np.where(score == target - 1)[0]) == 0:\n",
        "                mrr10.append(0)\n",
        "            else:\n",
        "                mrr10.append(1 / (10-np.where(score == target - 1)[0][0]))\n",
        "\n",
        "        for score, target in zip(index5, targets):\n",
        "            hit5.append(np.isin(target - 1, score))\n",
        "            if len(np.where(score == target - 1)[0]) == 0:\n",
        "                mrr5.append(0)\n",
        "            else:\n",
        "                mrr5.append(1 / (5-np.where(score == target - 1)[0][0]))\n",
        "    \n",
        "    hit20 = np.mean(hit20)*100\n",
        "    mrr20 = np.mean(mrr20)*100\n",
        "\n",
        "    hit10 = np.mean(hit10)*100\n",
        "    mrr10 = np.mean(mrr10)*100\n",
        "\n",
        "    hit5 = np.mean(hit5)*100\n",
        "    mrr5 = np.mean(mrr5)*100\n",
        "\n",
        "    print('Hit 5: '+str(hit5))\n",
        "    print('MRR 5: '+str(mrr5))\n",
        "\n",
        "    test_loss = np.mean(test_loss_)\n",
        "\n",
        "    if hit20 >= best_result[0]:\n",
        "        best_result[0] = hit20\n",
        "        best_epoch[0] = epoch\n",
        "    if mrr20 >= best_result[1]:\n",
        "        best_result[1] = mrr20\n",
        "        best_epoch[1]=epoch\n",
        "\n",
        "    if hit10 >= best_result[2]:\n",
        "        best_result[2] = hit10\n",
        "        best_epoch[2] = epoch\n",
        "    if mrr10 >= best_result[3]:\n",
        "        best_result[3] = mrr10\n",
        "        best_epoch[3]=epoch\n",
        "\n",
        "    if hit5 >= best_result[4]:\n",
        "        best_result[4] = hit5\n",
        "        best_epoch[4] = epoch\n",
        "    if mrr5 >= best_result[5]:\n",
        "        best_result[5] = mrr5\n",
        "        best_epoch[5]=epoch\n",
        "\n",
        "    print('train_loss:\\t%.4f\\ttest_loss:\\t%4f\\tHit@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d'\n",
        "    %\n",
        "          (loss, test_loss, best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n",
        "    print('Hit@10: '+str(best_result[2]) + '  MMR@10: '+str(best_result[3])+'  Hit@5: '+str(best_result[4]) + '  MMR@5: '+str(best_result[5]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tblKK0F6utFT"
      },
      "outputs": [],
      "source": [
        "def build_graph(train_data):\n",
        "    graph = nx.DiGraph()\n",
        "    for seq in train_data:\n",
        "        for i in range(len(seq) - 1):\n",
        "            if graph.get_edge_data(seq[i], seq[i + 1]) is None:\n",
        "                weight = 1\n",
        "            else:\n",
        "                weight = graph.get_edge_data(seq[i], seq[i + 1])['weight'] + 1\n",
        "            graph.add_edge(seq[i], seq[i + 1], weight=weight)\n",
        "    for node in graph.nodes:\n",
        "        sum = 0\n",
        "        for j, i in graph.in_edges(node):\n",
        "            sum += graph.get_edge_data(j, i)['weight']\n",
        "        if sum != 0:\n",
        "            for j, i in graph.in_edges(i):\n",
        "                graph.add_edge(j, i, weight=graph.get_edge_data(j, i)['weight'] / sum)\n",
        "    return graph\n",
        "\n",
        "\n",
        "def data_masks(all_usr_pois, item_tail):\n",
        "    us_lens = [len(upois) for upois in all_usr_pois]\n",
        "    len_max = max(us_lens)\n",
        "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
        "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
        "    return us_pois, us_msks, len_max\n",
        "\n",
        "\n",
        "def split_validation(train_set, valid_portion):\n",
        "    train_set_x, train_set_y = train_set\n",
        "    n_samples = len(train_set_x)\n",
        "    sidx = np.arange(n_samples, dtype='int32')\n",
        "    np.random.shuffle(sidx)\n",
        "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
        "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
        "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
        "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
        "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
        "\n",
        "    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)\n",
        "\n",
        "\n",
        "class Data():\n",
        "    def __init__(self, data, sub_graph=False, method='ggnn', sparse=False, shuffle=False):\n",
        "        inputs = data[0]\n",
        "        inputs, mask, len_max = data_masks(inputs, [0])\n",
        "        self.inputs = np.asarray(inputs)\n",
        "        self.mask = np.asarray(mask)\n",
        "        self.len_max = len_max\n",
        "        self.targets = np.asarray(data[1])\n",
        "        self.length = len(inputs)\n",
        "        self.shuffle = shuffle\n",
        "        self.sub_graph = sub_graph\n",
        "        self.sparse = sparse\n",
        "        self.method = method\n",
        "\n",
        "    def generate_batch(self, batch_size):\n",
        "        if self.shuffle:\n",
        "            shuffled_arg = np.arange(self.length)\n",
        "            np.random.shuffle(shuffled_arg)\n",
        "            self.inputs = self.inputs[shuffled_arg]\n",
        "            self.mask = self.mask[shuffled_arg]\n",
        "            self.targets = self.targets[shuffled_arg]\n",
        "        n_batch = int(self.length / batch_size)\n",
        "        if self.length % batch_size != 0:\n",
        "            n_batch += 1\n",
        "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
        "        slices[-1] = np.arange(self.length-batch_size, self.length)\n",
        "        return slices\n",
        "\n",
        "    def get_slice(self, index):\n",
        "        if 1:\n",
        "            items, n_node, A_in, A_out, alias_inputs = [], [], [], [], []\n",
        "            for u_input in self.inputs[index]:\n",
        "                n_node.append(len(np.unique(u_input)))\n",
        "            max_n_node = np.max(n_node)\n",
        "            if self.method == 'ggnn':\n",
        "                for u_input in self.inputs[index]:\n",
        "                    node = np.unique(u_input)\n",
        "                    items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
        "                    u_A = np.zeros((max_n_node, max_n_node))\n",
        "                    for i in np.arange(len(u_input) - 1):\n",
        "                        if u_input[i + 1] == 0:\n",
        "                            break\n",
        "                        u = np.where(node == u_input[i])[0][0]\n",
        "                        v = np.where(node == u_input[i + 1])[0][0]\n",
        "                        u_A[u][v] = 1\n",
        "                    u_sum_in = np.sum(u_A, 0)\n",
        "                    u_sum_in[np.where(u_sum_in == 0)] = 1\n",
        "                    u_A_in = np.divide(u_A, u_sum_in)\n",
        "                    u_sum_out = np.sum(u_A, 1)\n",
        "                    u_sum_out[np.where(u_sum_out == 0)] = 1\n",
        "                    u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
        "\n",
        "                    A_in.append(u_A_in)\n",
        "                    A_out.append(u_A_out)\n",
        "                    alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
        "                return A_in, A_out, alias_inputs, items, self.mask[index], self.targets[index]\n",
        "            elif self.method == 'gat':\n",
        "                A_in = []\n",
        "                A_out = []\n",
        "                for u_input in self.inputs[index]:\n",
        "                    node = np.unique(u_input)\n",
        "                    items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
        "                    u_A = np.eye(max_n_node)\n",
        "                    for i in np.arange(len(u_input) - 1):\n",
        "                        if u_input[i + 1] == 0:\n",
        "                            break\n",
        "                        u = np.where(node == u_input[i])[0][0]\n",
        "                        v = np.where(node == u_input[i + 1])[0][0]\n",
        "                        u_A[u][v] = 1\n",
        "                    A_in.append(-1e9 * (1 - u_A))\n",
        "                    A_out.append(-1e9 * (1 - u_A.transpose()))\n",
        "                    alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
        "                return A_in, A_out, alias_inputs, items, self.mask[index], self.targets[index]\n",
        "\n",
        "        else:\n",
        "            return self.inputs[index], self.mask[index], self.targets[index]\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, hidden_size=100, out_size=100, batch_size=100, nonhybrid=True):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.out_size = out_size\n",
        "        self.batch_size = batch_size\n",
        "        self.mask = tf.placeholder(dtype=tf.float32)\n",
        "        self.alias = tf.placeholder(dtype=tf.int32)  # 给给每个输入重新\n",
        "        self.item = tf.placeholder(dtype=tf.int32)   # 重新编号的序列构成的矩阵\n",
        "        self.tar = tf.placeholder(dtype=tf.int32)\n",
        "        self.nonhybrid = nonhybrid\n",
        "        self.stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "\n",
        "        self.nasr_w1 = tf.get_variable('nasr_w1', [self.out_size, self.out_size], dtype=tf.float32,\n",
        "                                       initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
        "        self.nasr_w2 = tf.get_variable('nasr_w2', [self.out_size, self.out_size], dtype=tf.float32,\n",
        "                                       initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
        "        self.nasr_v = tf.get_variable('nasrv', [1, self.out_size], dtype=tf.float32,\n",
        "                                      initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
        "        self.nasr_b = tf.get_variable('nasr_b', [self.out_size], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
        "\n",
        "    def forward(self, re_embedding, train=True):\n",
        "        rm = tf.reduce_sum(self.mask, 1)\n",
        "        last_id = tf.gather_nd(self.alias, tf.stack([tf.range(self.batch_size), tf.to_int32(rm)-1], axis=1))\n",
        "        last_h = tf.gather_nd(re_embedding, tf.stack([tf.range(self.batch_size), last_id], axis=1))\n",
        "        seq_h = tf.stack([tf.nn.embedding_lookup(re_embedding[i], self.alias[i]) for i in range(self.batch_size)],\n",
        "                         axis=0)                                                           #batch_size*T*d\n",
        "        last = tf.matmul(last_h, self.nasr_w1)\n",
        "        seq = tf.matmul(tf.reshape(seq_h, [-1, self.out_size]), self.nasr_w2)\n",
        "        last = tf.reshape(last, [self.batch_size, 1, -1])\n",
        "        m = tf.nn.sigmoid(last + tf.reshape(seq, [self.batch_size, -1, self.out_size]) + self.nasr_b)\n",
        "        coef = tf.matmul(tf.reshape(m, [-1, self.out_size]), self.nasr_v, transpose_b=True) * tf.reshape(\n",
        "            self.mask, [-1, 1])\n",
        "        b = self.embedding[1:]\n",
        "        if not self.nonhybrid:\n",
        "            ma = tf.concat([tf.reduce_sum(tf.reshape(coef, [self.batch_size, -1, 1]) * seq_h, 1),\n",
        "                            tf.reshape(last, [-1, self.out_size])], -1)\n",
        "            self.B = tf.get_variable('B', [2 * self.out_size, self.out_size],\n",
        "                                     initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
        "            y1 = tf.matmul(ma, self.B)\n",
        "            logits = tf.matmul(y1, b, transpose_b=True)\n",
        "        else:\n",
        "            ma = tf.reduce_sum(tf.reshape(coef, [self.batch_size, -1, 1]) * seq_h, 1)\n",
        "            logits = tf.matmul(ma, b, transpose_b=True)\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.tar - 1, logits=logits))\n",
        "        self.vars = tf.trainable_variables()\n",
        "        if train:\n",
        "            lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in self.vars if v.name not\n",
        "                               in ['bias', 'gamma', 'b', 'g', 'beta']]) * self.L2\n",
        "            loss = loss + lossL2\n",
        "        return loss, logits\n",
        "\n",
        "    def run(self, fetches, tar, item, adj_in, adj_out, alias, mask):\n",
        "        return self.sess.run(fetches, feed_dict={self.tar: tar, self.item: item, self.adj_in: adj_in,\n",
        "                                                 self.adj_out: adj_out, self.alias: alias, self.mask: mask})\n",
        "\n",
        "\n",
        "class GGNN(Model):\n",
        "    def __init__(self,hidden_size=100, out_size=100, batch_size=300, n_node=None,\n",
        "                 lr=None, l2=None, step=1, decay=None, lr_dc=0.1, nonhybrid=False):\n",
        "        super(GGNN,self).__init__(hidden_size, out_size, batch_size, nonhybrid)\n",
        "        self.embedding = tf.get_variable(shape=[n_node, hidden_size], name='embedding', dtype=tf.float32,\n",
        "                                         initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
        "        self.adj_in = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, None, None])\n",
        "        self.adj_out = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, None, None])\n",
        "        self.n_node = n_node\n",
        "        self.L2 = l2\n",
        "        self.step = step\n",
        "        self.nonhybrid = nonhybrid\n",
        "        self.W_in = tf.get_variable('W_in', shape=[self.out_size, self.out_size], dtype=tf.float32,\n",
        "                                    initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
        "        self.b_in = tf.get_variable('b_in', [self.out_size], dtype=tf.float32,\n",
        "                                    initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
        "        self.W_out = tf.get_variable('W_out', [self.out_size, self.out_size], dtype=tf.float32,\n",
        "                                     initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
        "        self.b_out = tf.get_variable('b_out', [self.out_size], dtype=tf.float32,\n",
        "                                     initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
        "        with tf.variable_scope('ggnn_model', reuse=None):\n",
        "            self.loss_train, _ = self.forward(self.ggnn())\n",
        "        with tf.variable_scope('ggnn_model', reuse=True):\n",
        "            self.loss_test, self.score_test = self.forward(self.ggnn(), train=False)\n",
        "        self.global_step = tf.Variable(0)\n",
        "        self.learning_rate = tf.train.exponential_decay(lr, global_step=self.global_step, decay_steps=decay,\n",
        "                                                        decay_rate=lr_dc, staircase=True)\n",
        "        self.opt = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss_train, global_step=self.global_step)\n",
        "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
        "        config = tf.ConfigProto(gpu_options=gpu_options)\n",
        "        config.gpu_options.allow_growth = True\n",
        "        self.sess = tf.Session(config=config)\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def ggnn(self):\n",
        "        fin_state = tf.nn.embedding_lookup(self.embedding, self.item)\n",
        "        cell = tf.nn.rnn_cell.GRUCell(self.out_size)\n",
        "        with tf.variable_scope('gru'):\n",
        "            for i in range(self.step):\n",
        "                fin_state = tf.reshape(fin_state, [self.batch_size, -1, self.out_size])\n",
        "                fin_state_in = tf.reshape(tf.matmul(tf.reshape(fin_state, [-1, self.out_size]),\n",
        "                                                    self.W_in) + self.b_in, [self.batch_size, -1, self.out_size])\n",
        "                fin_state_out = tf.reshape(tf.matmul(tf.reshape(fin_state, [-1, self.out_size]),\n",
        "                                                     self.W_out) + self.b_out, [self.batch_size, -1, self.out_size])\n",
        "                av = tf.concat([tf.matmul(self.adj_in, fin_state_in),\n",
        "                                tf.matmul(self.adj_out, fin_state_out)], axis=-1)\n",
        "                state_output, fin_state = \\\n",
        "                    tf.nn.dynamic_rnn(cell, tf.expand_dims(tf.reshape(av, [-1, 2*self.out_size]), axis=1),\n",
        "                                      initial_state=tf.reshape(fin_state, [-1, self.out_size]))\n",
        "        return tf.reshape(fin_state, [self.batch_size, -1, self.out_size])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f0zcovyWuwRD"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python36\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on July, 2018\n",
        "@author: Tangrizzly\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import csv\n",
        "import pickle\n",
        "import operator\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "\"\"\"parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataset', default='sample', help='dataset name: diginetica/yoochoose/sample')\n",
        "opt = parser.parse_args()\n",
        "print(opt)\n",
        "\n",
        "dataset = 'sample_train-item-views.csv'\n",
        "if opt.dataset == 'diginetica':\n",
        "    dataset = 'train-item-views.csv'\n",
        "elif opt.dataset =='yoochoose':\n",
        "    dataset = 'yoochoose-clicks.dat'\"\"\"\n",
        "\n",
        "print(\"-- Starting @ %ss\" % datetime.datetime.now())\n",
        "with open('train-item-views.csv', \"r\") as f:\n",
        "    reader = csv.DictReader(f, delimiter=';')\n",
        "    sess_clicks = {}\n",
        "    sess_date = {}\n",
        "    ctr = 0\n",
        "    curid = -1\n",
        "    curdate = None\n",
        "    for data in reader:\n",
        "        print(data)\n",
        "        sessid = data['sessionId']\n",
        "        if curdate and not curid == sessid:\n",
        "            date = ''\n",
        "            date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n",
        "            sess_date[curid] = date\n",
        "        curid = sessid\n",
        "        item = data['itemId'], int(data['timeframe'])\n",
        "        curdate = ''\n",
        "        curdate = data['eventdate']\n",
        "\n",
        "        if sessid in sess_clicks:\n",
        "            sess_clicks[sessid] += [item]\n",
        "        else:\n",
        "            sess_clicks[sessid] = [item]\n",
        "        ctr += 1\n",
        "    date = ''\n",
        "    date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n",
        "    for i in list(sess_clicks):\n",
        "        sorted_clicks = sorted(sess_clicks[i], key=operator.itemgetter(1))\n",
        "        sess_clicks[i] = [c[0] for c in sorted_clicks]\n",
        "    sess_date[curid] = date\n",
        "print(\"-- Reading data @ %ss\" % datetime.datetime.now())\n",
        "\n",
        "# Filter out length 1 sessions\n",
        "for s in list(sess_clicks):\n",
        "    if len(sess_clicks[s]) == 1:\n",
        "        del sess_clicks[s]\n",
        "        del sess_date[s]\n",
        "\n",
        "# Count number of times each item appears\n",
        "iid_counts = {}\n",
        "for s in sess_clicks:\n",
        "    seq = sess_clicks[s]\n",
        "    for iid in seq:\n",
        "        if iid in iid_counts:\n",
        "            iid_counts[iid] += 1\n",
        "        else:\n",
        "            iid_counts[iid] = 1\n",
        "\n",
        "sorted_counts = sorted(iid_counts.items(), key=operator.itemgetter(1))\n",
        "\n",
        "length = len(sess_clicks)\n",
        "for s in list(sess_clicks):\n",
        "    curseq = sess_clicks[s]\n",
        "    filseq = list(filter(lambda i: iid_counts[i] >= 5, curseq))\n",
        "    if len(filseq) < 2:\n",
        "        del sess_clicks[s]\n",
        "        del sess_date[s]\n",
        "    else:\n",
        "        sess_clicks[s] = filseq\n",
        "\n",
        "# Split out test set based on dates\n",
        "dates = list(sess_date.items())\n",
        "maxdate = dates[0][1]\n",
        "\n",
        "for _, date in dates:\n",
        "    if maxdate < date:\n",
        "        maxdate = date\n",
        "\n",
        "# 7 days for test\n",
        "splitdate = 0\n",
        "splitdate = maxdate - 86400 * 7\n",
        "\n",
        "print('Splitting date', splitdate)      # Yoochoose: ('Split date', 1411930799.0)\n",
        "tra_sess = filter(lambda x: x[1] < splitdate, dates)\n",
        "tes_sess = filter(lambda x: x[1] > splitdate, dates)\n",
        "\n",
        "# Sort sessions by date\n",
        "tra_sess = sorted(tra_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n",
        "tes_sess = sorted(tes_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n",
        "print(len(tra_sess))    # 186670    # 7966257\n",
        "print(len(tes_sess))    # 15979     # 15324\n",
        "print(tra_sess[:3])\n",
        "print(tes_sess[:3])\n",
        "print(\"-- Splitting train set and test set @ %ss\" % datetime.datetime.now())\n",
        "\n",
        "# Choosing item count >=5 gives approximately the same number of items as reported in paper\n",
        "item_dict = {}\n",
        "# Convert training sessions to sequences and renumber items to start from 1\n",
        "def obtian_tra():\n",
        "    train_ids = []\n",
        "    train_seqs = []\n",
        "    train_dates = []\n",
        "    item_ctr = 1\n",
        "    for s, date in tra_sess:\n",
        "        seq = sess_clicks[s]\n",
        "        outseq = []\n",
        "        for i in seq:\n",
        "            if i in item_dict:\n",
        "                outseq += [item_dict[i]]\n",
        "            else:\n",
        "                outseq += [item_ctr]\n",
        "                item_dict[i] = item_ctr\n",
        "                item_ctr += 1\n",
        "        if len(outseq) < 2:  # Doesn't occur\n",
        "            continue\n",
        "        train_ids += [s]\n",
        "        train_dates += [date]\n",
        "        train_seqs += [outseq]\n",
        "    print(item_ctr)     # 43098, 37484\n",
        "    return train_ids, train_dates, train_seqs\n",
        "\n",
        "\n",
        "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
        "def obtian_tes():\n",
        "    test_ids = []\n",
        "    test_seqs = []\n",
        "    test_dates = []\n",
        "    for s, date in tes_sess:\n",
        "        seq = sess_clicks[s]\n",
        "        outseq = []\n",
        "        for i in seq:\n",
        "            if i in item_dict:\n",
        "                outseq += [item_dict[i]]\n",
        "        if len(outseq) < 2:\n",
        "            continue\n",
        "        test_ids += [s]\n",
        "        test_dates += [date]\n",
        "        test_seqs += [outseq]\n",
        "    return test_ids, test_dates, test_seqs\n",
        "\n",
        "\n",
        "tra_ids, tra_dates, tra_seqs = obtian_tra()\n",
        "tes_ids, tes_dates, tes_seqs = obtian_tes()\n",
        "\n",
        "\n",
        "def process_seqs(iseqs, idates):\n",
        "    out_seqs = []\n",
        "    out_dates = []\n",
        "    labs = []\n",
        "    ids = []\n",
        "    for id, seq, date in zip(range(len(iseqs)), iseqs, idates):\n",
        "        for i in range(1, len(seq)):\n",
        "            tar = seq[-i]\n",
        "            labs += [tar]\n",
        "            out_seqs += [seq[:-i]]\n",
        "            out_dates += [date]\n",
        "            ids += [id]\n",
        "    return out_seqs, out_dates, labs, ids\n",
        "\n",
        "\n",
        "tr_seqs, tr_dates, tr_labs, tr_ids = process_seqs(tra_seqs, tra_dates)\n",
        "te_seqs, te_dates, te_labs, te_ids = process_seqs(tes_seqs, tes_dates)\n",
        "tra = (tr_seqs, tr_labs)\n",
        "tes = (te_seqs, te_labs)\n",
        "print(len(tr_seqs))\n",
        "print(len(te_seqs))\n",
        "print(tr_seqs[:3], tr_dates[:3], tr_labs[:3])\n",
        "print(te_seqs[:3], te_dates[:3], te_labs[:3])\n",
        "all = 0\n",
        "\n",
        "for seq in tra_seqs:\n",
        "    all += len(seq)\n",
        "for seq in tes_seqs:\n",
        "    all += len(seq)\n",
        "print('avg length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
        "\n",
        "if not os.path.exists('sample'):\n",
        "    os.makedirs('sample')\n",
        "pickle.dump(tra, open('sample/train.txt', 'wb'))\n",
        "pickle.dump(tes, open('sample/test.txt', 'wb'))\n",
        "pickle.dump(tra_seqs, open('sample/all_train_seq.txt', 'wb'))\n",
        "\n",
        "print('Done.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SR-GNN",
      "provenance": [],
      "authorship_tag": "ABX9TyODUMlJgeeuEl67CMB5KLGn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}