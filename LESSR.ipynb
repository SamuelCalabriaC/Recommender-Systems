{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LESSR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCpZit6HoLA3J/ddZkHd+4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelCalabriaC/Recommender-Systems/blob/main/LESSR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ18Kd5F3uKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9d927c-e38e-4a03-f5ae-dfd31fd5d297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.1\n",
            "Collecting dgl-cu111\n",
            "  Downloading dgl_cu111-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (41.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.0 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.6.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2.10)\n",
            "Installing collected packages: dgl-cu111\n",
            "Successfully installed dgl-cu111-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl\n",
        "!pip install dgl-cu111"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPU0djbi6Ddm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a93f9e-dc8d-4455-eeec-b5d8cdf0d96c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch as th\n",
        "from torch import nn\n",
        "import dgl\n",
        "import dgl.ops as F\n",
        "import dgl.function as fn\n",
        "\n",
        "\n",
        "class EOPA(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_dim, output_dim, batch_norm=True, feat_drop=0.0, activation=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.gru = nn.GRU(input_dim, input_dim, batch_first=True)\n",
        "        self.fc_self = nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.fc_neigh = nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.activation = activation\n",
        "\n",
        "    def reducer(self, nodes):\n",
        "        m = nodes.mailbox['m']  # (num_nodes, deg, d)\n",
        "        # m[i]: the messages passed to the i-th node with in-degree equal to 'deg'\n",
        "        # the order of messages follows the order of incoming edges\n",
        "        # since the edges are sorted by occurrence time when the EOP multigraph is built\n",
        "        # the messages are in the order required by EOPA\n",
        "        _, hn = self.gru(m)  # hn: (1, num_nodes, d)\n",
        "        return {'neigh': hn.squeeze(0)}\n",
        "\n",
        "    def forward(self, mg, feat):\n",
        "        with mg.local_scope():\n",
        "            if self.batch_norm is not None:\n",
        "                feat = self.batch_norm(feat)\n",
        "            mg.ndata['ft'] = self.feat_drop(feat)\n",
        "            if mg.number_of_edges() > 0:\n",
        "                mg.update_all(fn.copy_u('ft', 'm'), self.reducer)\n",
        "                neigh = mg.ndata['neigh']\n",
        "                rst = self.fc_self(feat) + self.fc_neigh(neigh)\n",
        "            else:\n",
        "                rst = self.fc_self(feat)\n",
        "            if self.activation is not None:\n",
        "                rst = self.activation(rst)\n",
        "            return rst\n",
        "\n",
        "\n",
        "class SGAT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        hidden_dim,\n",
        "        output_dim,\n",
        "        batch_norm=True,\n",
        "        feat_drop=0.0,\n",
        "        activation=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.fc_q = nn.Linear(input_dim, hidden_dim, bias=True)\n",
        "        self.fc_k = nn.Linear(input_dim, hidden_dim, bias=False)\n",
        "        self.fc_v = nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.fc_e = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, sg, feat):\n",
        "        if self.batch_norm is not None:\n",
        "            feat = self.batch_norm(feat)\n",
        "        feat = self.feat_drop(feat)\n",
        "        q = self.fc_q(feat)\n",
        "        k = self.fc_k(feat)\n",
        "        v = self.fc_v(feat)\n",
        "        e = F.u_add_v(sg, q, k)\n",
        "        e = self.fc_e(th.sigmoid(e))\n",
        "        a = F.edge_softmax(sg, e)\n",
        "        rst = F.u_mul_e_sum(sg, v, a)\n",
        "        if self.activation is not None:\n",
        "            rst = self.activation(rst)\n",
        "        return rst\n",
        "\n",
        "\n",
        "class AttnReadout(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        hidden_dim,\n",
        "        output_dim,\n",
        "        batch_norm=True,\n",
        "        feat_drop=0.0,\n",
        "        activation=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.fc_u = nn.Linear(input_dim, hidden_dim, bias=False)\n",
        "        self.fc_v = nn.Linear(input_dim, hidden_dim, bias=True)\n",
        "        self.fc_e = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        self.fc_out = (\n",
        "            nn.Linear(input_dim, output_dim, bias=False)\n",
        "            if output_dim != input_dim else None\n",
        "        )\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, g, feat, last_nodes):\n",
        "        if self.batch_norm is not None:\n",
        "            feat = self.batch_norm(feat)\n",
        "        feat = self.feat_drop(feat)\n",
        "        feat_u = self.fc_u(feat)\n",
        "        feat_v = self.fc_v(feat[last_nodes])\n",
        "        feat_v = dgl.broadcast_nodes(g, feat_v)\n",
        "        e = self.fc_e(th.sigmoid(feat_u + feat_v))\n",
        "        alpha = F.segment.segment_softmax(g.batch_num_nodes(), e)\n",
        "        feat_norm = feat * alpha\n",
        "        rst = F.segment.segment_reduce(g.batch_num_nodes(), feat_norm, 'sum')\n",
        "        if self.fc_out is not None:\n",
        "            rst = self.fc_out(rst)\n",
        "        if self.activation is not None:\n",
        "            rst = self.activation(rst)\n",
        "        return rst\n",
        "\n",
        "\n",
        "class LESSR(nn.Module):\n",
        "    def __init__(\n",
        "        self, num_items, embedding_dim, num_layers, batch_norm=True, feat_drop=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_items, embedding_dim, max_norm=1)\n",
        "        self.indices = nn.Parameter(\n",
        "            th.arange(num_items, dtype=th.long), requires_grad=False\n",
        "        )\n",
        "        self.num_layers = num_layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        input_dim = embedding_dim\n",
        "        for i in range(num_layers):\n",
        "            if i % 2 == 0:\n",
        "                layer = EOPA(\n",
        "                    input_dim,\n",
        "                    embedding_dim,\n",
        "                    batch_norm=batch_norm,\n",
        "                    feat_drop=feat_drop,\n",
        "                    activation=nn.PReLU(embedding_dim),\n",
        "                )\n",
        "            else:\n",
        "                layer = SGAT(\n",
        "                    input_dim,\n",
        "                    embedding_dim,\n",
        "                    embedding_dim,\n",
        "                    batch_norm=batch_norm,\n",
        "                    feat_drop=feat_drop,\n",
        "                    activation=nn.PReLU(embedding_dim),\n",
        "                )\n",
        "            input_dim += embedding_dim\n",
        "            self.layers.append(layer)\n",
        "        self.readout = AttnReadout(\n",
        "            input_dim,\n",
        "            embedding_dim,\n",
        "            embedding_dim,\n",
        "            batch_norm=batch_norm,\n",
        "            feat_drop=feat_drop,\n",
        "            activation=nn.PReLU(embedding_dim),\n",
        "        )\n",
        "        input_dim += embedding_dim\n",
        "        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.fc_sr = nn.Linear(input_dim, embedding_dim, bias=False)\n",
        "\n",
        "    def forward(self, mg, sg=None):\n",
        "        iid = mg.ndata['iid']\n",
        "        feat = self.embedding(iid)\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i % 2 == 0:\n",
        "                out = layer(mg, feat)\n",
        "            else:\n",
        "                out = layer(sg, feat)\n",
        "            feat = th.cat([out, feat], dim=1)\n",
        "        last_nodes = mg.filter_nodes(lambda nodes: nodes.data['last'] == 1)\n",
        "        sr_g = self.readout(mg, feat, last_nodes)\n",
        "        sr_l = feat[last_nodes]\n",
        "        sr = th.cat([sr_l, sr_g], dim=1)\n",
        "        if self.batch_norm is not None:\n",
        "            sr = self.batch_norm(sr)\n",
        "        sr = self.fc_sr(self.feat_drop(sr))\n",
        "        logits = sr @ self.embedding(self.indices).t()\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch as th\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "# ignore weight decay for parameters in bias, batch norm and activation\n",
        "def fix_weight_decay(model):\n",
        "    decay = []\n",
        "    no_decay = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if any(map(lambda x: x in name, ['bias', 'batch_norm', 'activation'])):\n",
        "            no_decay.append(param)\n",
        "        else:\n",
        "            decay.append(param)\n",
        "    params = [{'params': decay}, {'params': no_decay, 'weight_decay': 0}]\n",
        "    return params\n",
        "\n",
        "\n",
        "def prepare_batch(batch, device):\n",
        "    inputs, labels = batch\n",
        "    inputs_gpu = [x.to(device) for x in inputs]\n",
        "    labels_gpu = labels.to(device)\n",
        "    return inputs_gpu, labels_gpu\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, device, Ks=[20]):\n",
        "    model.eval()\n",
        "    num_samples = 0\n",
        "    max_K = max(Ks)\n",
        "    results = defaultdict(float)\n",
        "    with th.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputs, labels = prepare_batch(batch, device)\n",
        "            logits = model(*inputs)\n",
        "            batch_size = logits.size(0)\n",
        "            num_samples += batch_size\n",
        "            topk = th.topk(logits, k=max_K, sorted=True)[1]\n",
        "            labels = labels.unsqueeze(-1)\n",
        "            for K in Ks:\n",
        "                hit_ranks = th.where(topk[:, :K] == labels)[1] + 1\n",
        "                hit_ranks = hit_ranks.float().cpu()\n",
        "                results[f'HR@{K}'] += hit_ranks.numel()\n",
        "                results[f'MRR@{K}'] += hit_ranks.reciprocal().sum().item()\n",
        "                results[f'NDCG@{K}'] += th.log2(1 + hit_ranks).reciprocal().sum().item()\n",
        "    for metric in results:\n",
        "        results[metric] /= num_samples\n",
        "    return results\n",
        "\n",
        "\n",
        "def print_results(results, epochs=None):\n",
        "    print('Metric\\t' + '\\t'.join(results.keys()))\n",
        "    print(\n",
        "        'Value\\t' +\n",
        "        '\\t'.join([f'{round(val * 100, 2):.2f}' for val in results.values()])\n",
        "    )\n",
        "    if epochs is not None:\n",
        "        print('Epoch\\t' + '\\t'.join([str(epochs[metric]) for metric in results]))\n",
        "\n",
        "\n",
        "class TrainRunner:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        train_loader,\n",
        "        test_loader,\n",
        "        device,\n",
        "        lr=1e-3,\n",
        "        weight_decay=0,\n",
        "        patience=3,\n",
        "        Ks=[20],\n",
        "    ):\n",
        "        self.model = model\n",
        "        if weight_decay > 0:\n",
        "            params = fix_weight_decay(model)\n",
        "        else:\n",
        "            params = model.parameters()\n",
        "        self.optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.epoch = 0\n",
        "        self.batch = 0\n",
        "        self.patience = patience\n",
        "        self.Ks = Ks\n",
        "\n",
        "    def train(self, epochs, log_interval=100):\n",
        "        max_results = defaultdict(float)\n",
        "        max_epochs = defaultdict(int)\n",
        "        bad_counter = 0\n",
        "        t = time.time()\n",
        "        mean_loss = 0\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            for batch in self.train_loader:\n",
        "                inputs, labels = prepare_batch(batch, self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                logits = self.model(*inputs)\n",
        "                loss = nn.functional.cross_entropy(logits, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                mean_loss += loss.item() / log_interval\n",
        "                if self.batch > 0 and self.batch % log_interval == 0:\n",
        "                    print(\n",
        "                        f'Batch {self.batch}: Loss = {mean_loss:.4f}, Time Elapsed = {time.time() - t:.2f}s'\n",
        "                    )\n",
        "                    t = time.time()\n",
        "                    mean_loss = 0\n",
        "                self.batch += 1\n",
        "\n",
        "            curr_results = evaluate(\n",
        "                self.model, self.test_loader, self.device, Ks=self.Ks\n",
        "            )\n",
        "\n",
        "            print(f'\\nEpoch {self.epoch}:')\n",
        "            print_results(curr_results)\n",
        "\n",
        "            any_better_result = False\n",
        "            for metric in curr_results:\n",
        "                if curr_results[metric] > max_results[metric]:\n",
        "                    max_results[metric] = curr_results[metric]\n",
        "                    max_epochs[metric] = self.epoch\n",
        "                    any_better_result = True\n",
        "\n",
        "            if any_better_result:\n",
        "                bad_counter = 0\n",
        "            else:\n",
        "                bad_counter += 1\n",
        "                if bad_counter == self.patience:\n",
        "                    break\n",
        "\n",
        "            self.epoch += 1\n",
        "        print('\\nBest results')\n",
        "        print_results(max_results, max_epochs)\n",
        "        return max_results"
      ],
      "metadata": {
        "id": "xsMVCFIF6KO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import dgl\n",
        "\n",
        "\n",
        "def label_last(g, last_nid):\n",
        "    is_last = th.zeros(g.number_of_nodes(), dtype=th.int32)\n",
        "    is_last[last_nid] = 1\n",
        "    g.ndata['last'] = is_last\n",
        "    return g\n",
        "\n",
        "\n",
        "def seq_to_eop_multigraph(seq):\n",
        "    items = np.unique(seq)\n",
        "    iid2nid = {iid: i for i, iid in enumerate(items)}\n",
        "    num_nodes = len(items)\n",
        "\n",
        "    if len(seq) > 1:\n",
        "        seq_nid = [iid2nid[iid] for iid in seq]\n",
        "        src = seq_nid[:-1]\n",
        "        dst = seq_nid[1:]\n",
        "    else:\n",
        "        src = th.LongTensor([])\n",
        "        dst = th.LongTensor([])\n",
        "    g = dgl.graph((src, dst), num_nodes=num_nodes)\n",
        "    g.ndata['iid'] = th.tensor(items, dtype=th.long)\n",
        "    label_last(g, iid2nid[seq[-1]])\n",
        "    return g\n",
        "\n",
        "\n",
        "def seq_to_shortcut_graph(seq):\n",
        "    items = np.unique(seq)\n",
        "    iid2nid = {iid: i for i, iid in enumerate(items)}\n",
        "    num_nodes = len(items)\n",
        "\n",
        "    seq_nid = [iid2nid[iid] for iid in seq]\n",
        "    counter = Counter(\n",
        "        [(seq_nid[i], seq_nid[j]) for i in range(len(seq)) for j in range(i, len(seq))]\n",
        "    )\n",
        "    edges = counter.keys()\n",
        "    src, dst = zip(*edges)\n",
        "\n",
        "    g = dgl.graph((src, dst), num_nodes=num_nodes)\n",
        "    return g\n",
        "\n",
        "\n",
        "def collate_fn_factory(*seq_to_graph_fns):\n",
        "    def collate_fn(samples):\n",
        "        seqs, labels = zip(*samples)\n",
        "        inputs = []\n",
        "        for seq_to_graph in seq_to_graph_fns:\n",
        "            graphs = list(map(seq_to_graph, seqs))\n",
        "            bg = dgl.batch(graphs)\n",
        "            inputs.append(bg)\n",
        "        labels = th.LongTensor(labels)\n",
        "        return inputs, labels\n",
        "\n",
        "    return collate_fn"
      ],
      "metadata": {
        "id": "4oUdXWuJ79wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def create_index(sessions):\n",
        "    lens = np.fromiter(map(len, sessions), dtype=np.long)\n",
        "    session_idx = np.repeat(np.arange(len(sessions)), lens - 1)\n",
        "    label_idx = map(lambda l: range(1, l), lens)\n",
        "    label_idx = itertools.chain.from_iterable(label_idx)\n",
        "    label_idx = np.fromiter(label_idx, dtype=np.long)\n",
        "    idx = np.column_stack((session_idx, label_idx))\n",
        "    return idx\n",
        "\n",
        "\n",
        "def read_sessions(filepath):\n",
        "    sessions = pd.read_csv(filepath, sep='\\t', header=None, squeeze=True)\n",
        "    sessions = sessions.apply(lambda x: list(map(int, x.split(',')))).values\n",
        "    return sessions\n",
        "\n",
        "\n",
        "def read_dataset(dataset_dir):\n",
        "    train_sessions = read_sessions('train.txt')\n",
        "    test_sessions = read_sessions('test.txt')\n",
        "    with open('num_items.txt', 'r') as f:\n",
        "        num_items = int(f.readline())\n",
        "    return train_sessions, test_sessions, num_items\n",
        "\n",
        "\n",
        "class AugmentedDataset:\n",
        "    def __init__(self, sessions, sort_by_length=True):\n",
        "        self.sessions = sessions\n",
        "        index = create_index(sessions)  # columns: sessionId, labelIndex\n",
        "        if sort_by_length:\n",
        "            # sort by labelIndex in descending order\n",
        "            ind = np.argsort(index[:, 1])[::-1]\n",
        "            index = index[ind]\n",
        "        self.index = index\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sid, lidx = self.index[idx]\n",
        "        seq = self.sessions[sid][:lidx]\n",
        "        label = self.sessions[sid][lidx]\n",
        "        return seq, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)"
      ],
      "metadata": {
        "id": "2xvy4LlJ8Eim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "parser.add_argument('--dataset-dir', default='datasets/sample', help='the dataset directory')\n",
        "parser.add_argument('--embedding-dim', type=int, default=32, help='the embedding size')\n",
        "parser.add_argument('--num-layers', type=int, default=3, help='the number of layers')\n",
        "parser.add_argument('--feat-drop', type=float, default=0.2, help='the dropout ratio for features')\n",
        "parser.add_argument('--lr', type=float, default=1e-3, help='the learning rate')\n",
        "parser.add_argument('--batch-size', type=int, default=512, help='the batch size for training')\n",
        "parser.add_argument('--epochs', type=int, default=30, help='the number of training epochs')\n",
        "parser.add_argument('--weight-decay',type=float,default=1e-4,help='the parameter for L2 regularization',)\n",
        "parser.add_argument('--Ks',default='10,20',help='the values of K in evaluation metrics, separated by commas',)\n",
        "parser.add_argument('--patience',type=int,default=2,help='the number of epochs that the performance does not improves after which the training stops',)\n",
        "parser.add_argument('--num-workers',type=int,default=4,help='the number of processes to load the input graphs',)\n",
        "parser.add_argument('--valid-split',type=float,default=None,help='the fraction for the validation set',)\n",
        "parser.add_argument('--log-interval',type=int,default=100,help='print the loss after this number of iterations',)\n",
        "args = parser.parse_args()\n",
        "print(args)\"\"\"\n",
        "\n",
        "import gc\n",
        "import dgl\n",
        "from pathlib import Path\n",
        "import torch as th\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "ks = [5,10,20]\n",
        "print('reading dataset')\n",
        "train_sessions, test_sessions, num_items = read_dataset('.')\n",
        "\n",
        "train_set = AugmentedDataset(train_sessions)\n",
        "test_set = AugmentedDataset(test_sessions)\n",
        "\n",
        "if 3 > 1:\n",
        "    collate_fn = collate_fn_factory(seq_to_eop_multigraph, seq_to_shortcut_graph)\n",
        "else:\n",
        "    collate_fn = collate_fn_factory(seq_to_eop_multigraph)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_set,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "model = LESSR(num_items, 32,3, feat_drop=0.2)\n",
        "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "runner = TrainRunner(\n",
        "    model,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    device=device,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=2,\n",
        "    Ks=ks,\n",
        ")\n",
        "\n",
        "print('start training')\n",
        "runner.train(30, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "WMqyTush64QV",
        "outputId": "132aeda8-7653-4820-b770-dc03f6e9a467"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8032eb6f39c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dgl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}