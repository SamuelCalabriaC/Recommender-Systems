{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreprocessDiginetica.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPneZHRMgk2JolWafUOc7JC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelCalabriaC/Recommender-Systems/blob/main/PreprocessDiginetica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl\n",
        "!pip install dgl-cu111"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu9In71oqKFs",
        "outputId": "b6cfec05-bb8e-417c-9f39-92c0e05fb5d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 29.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.1\n",
            "Collecting dgl-cu111\n",
            "  Downloading dgl_cu111-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (41.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.0 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.6.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2.10)\n",
            "Installing collected packages: dgl-cu111\n",
            "Successfully installed dgl-cu111-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import dgl\n",
        "\n",
        "\n",
        "def label_last(g, last_nid):\n",
        "    is_last = th.zeros(g.number_of_nodes(), dtype=th.int32)\n",
        "    is_last[last_nid] = 1\n",
        "    g.ndata['last'] = is_last\n",
        "    return g\n",
        "\n",
        "\n",
        "def seq_to_eop_multigraph(seq):\n",
        "    items = np.unique(seq)\n",
        "    iid2nid = {iid: i for i, iid in enumerate(items)}\n",
        "    num_nodes = len(items)\n",
        "\n",
        "    if len(seq) > 1:\n",
        "        seq_nid = [iid2nid[iid] for iid in seq]\n",
        "        src = seq_nid[:-1]\n",
        "        dst = seq_nid[1:]\n",
        "    else:\n",
        "        src = th.LongTensor([])\n",
        "        dst = th.LongTensor([])\n",
        "    g = dgl.graph((src, dst), num_nodes=num_nodes)\n",
        "    g.ndata['iid'] = th.tensor(items, dtype=th.long)\n",
        "    label_last(g, iid2nid[seq[-1]])\n",
        "    return g\n",
        "\n",
        "\n",
        "def seq_to_shortcut_graph(seq):\n",
        "    items = np.unique(seq)\n",
        "    iid2nid = {iid: i for i, iid in enumerate(items)}\n",
        "    num_nodes = len(items)\n",
        "\n",
        "    seq_nid = [iid2nid[iid] for iid in seq]\n",
        "    counter = Counter(\n",
        "        [(seq_nid[i], seq_nid[j]) for i in range(len(seq)) for j in range(i, len(seq))]\n",
        "    )\n",
        "    edges = counter.keys()\n",
        "    src, dst = zip(*edges)\n",
        "\n",
        "    g = dgl.graph((src, dst), num_nodes=num_nodes)\n",
        "    return g\n",
        "\n",
        "\n",
        "def collate_fn_factory(*seq_to_graph_fns):\n",
        "    def collate_fn(samples):\n",
        "        seqs, labels = zip(*samples)\n",
        "        inputs = []\n",
        "        for seq_to_graph in seq_to_graph_fns:\n",
        "            graphs = list(map(seq_to_graph, seqs))\n",
        "            bg = dgl.batch(graphs)\n",
        "            inputs.append(bg)\n",
        "        labels = th.LongTensor(labels)\n",
        "        return inputs, labels\n",
        "\n",
        "    return collate_fn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLTx5S2FqC14",
        "outputId": "960fd9ca-b01d-46a7-fd99-78250aed9d53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def create_index(sessions):\n",
        "    lens = np.fromiter(map(len, sessions), dtype=np.long)\n",
        "    session_idx = np.repeat(np.arange(len(sessions)), lens - 1)\n",
        "    label_idx = map(lambda l: range(1, l), lens)\n",
        "    label_idx = itertools.chain.from_iterable(label_idx)\n",
        "    label_idx = np.fromiter(label_idx, dtype=np.long)\n",
        "    idx = np.column_stack((session_idx, label_idx))\n",
        "    return idx\n",
        "\n",
        "\n",
        "def read_sessions(filepath):\n",
        "    sessions = pd.read_csv(filepath, sep='\\t', header=None, squeeze=True)\n",
        "    sessions = sessions.apply(lambda x: list(map(int, x.split(',')))).values\n",
        "    return sessions\n",
        "\n",
        "\n",
        "def read_dataset(dataset_dir):\n",
        "    train_sessions = read_sessions(dataset_dir / 'train.txt')\n",
        "    test_sessions = read_sessions(dataset_dir / 'test.txt')\n",
        "    with open(dataset_dir / 'num_items.txt', 'r') as f:\n",
        "        num_items = int(f.readline())\n",
        "    return train_sessions, test_sessions, num_items\n",
        "\n",
        "\n",
        "class AugmentedDataset:\n",
        "    def __init__(self, sessions, sort_by_length=True):\n",
        "        self.sessions = sessions\n",
        "        index = create_index(sessions)  # columns: sessionId, labelIndex\n",
        "        if sort_by_length:\n",
        "            # sort by labelIndex in descending order\n",
        "            ind = np.argsort(index[:, 1])[::-1]\n",
        "            index = index[ind]\n",
        "        self.index = index\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sid, lidx = self.index[idx]\n",
        "        seq = self.sessions[sid][:lidx]\n",
        "        label = self.sessions[sid][lidx]\n",
        "        return seq, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)"
      ],
      "metadata": {
        "id": "FrUGms1np-a2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Is67c1xnpnJo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_session_id(df, interval):\n",
        "    df_prev = df.shift()\n",
        "    is_new_session = (df.userId != df_prev.userId) | (\n",
        "        df.timestamp - df_prev.timestamp > interval\n",
        "    )\n",
        "    session_id = is_new_session.cumsum() - 1\n",
        "    return session_id\n",
        "\n",
        "\n",
        "def group_sessions(df, interval):\n",
        "    sessionId = get_session_id(df, interval)\n",
        "    df = df.assign(sessionId=sessionId)\n",
        "    return df\n",
        "\n",
        "\n",
        "def filter_short_sessions(df, min_len=2):\n",
        "    session_len = df.groupby('sessionId', sort=False).size()\n",
        "    long_sessions = session_len[session_len >= min_len].index\n",
        "    df_long = df[df.sessionId.isin(long_sessions)]\n",
        "    return df_long\n",
        "\n",
        "\n",
        "def filter_infreq_items(df, min_support=5):\n",
        "    item_support = df.groupby('itemId', sort=False).size()\n",
        "    freq_items = item_support[item_support >= min_support].index\n",
        "    df_freq = df[df.itemId.isin(freq_items)]\n",
        "    return df_freq\n",
        "\n",
        "\n",
        "def filter_until_all_long_and_freq(df, min_len=2, min_support=5):\n",
        "    while True:\n",
        "        df_long = filter_short_sessions(df, min_len)\n",
        "        df_freq = filter_infreq_items(df_long, min_support)\n",
        "        if len(df_freq) == len(df):\n",
        "            break\n",
        "        df = df_freq\n",
        "    return df\n",
        "\n",
        "\n",
        "def truncate_long_sessions(df, max_len=20, is_sorted=False):\n",
        "    if not is_sorted:\n",
        "        df = df.sort_values(['sessionId', 'timestamp'])\n",
        "    itemIdx = df.groupby('sessionId').cumcount()\n",
        "    df_t = df[itemIdx < max_len]\n",
        "    return df_t\n",
        "\n",
        "\n",
        "def update_id(df, field):\n",
        "    labels = pd.factorize(df[field])[0]\n",
        "    kwargs = {field: labels}\n",
        "    df = df.assign(**kwargs)\n",
        "    return df\n",
        "\n",
        "\n",
        "def remove_immediate_repeats(df):\n",
        "    df_prev = df.shift()\n",
        "    is_not_repeat = (df.sessionId != df_prev.sessionId) | (df.itemId != df_prev.itemId)\n",
        "    df_no_repeat = df[is_not_repeat]\n",
        "    return df_no_repeat\n",
        "\n",
        "\n",
        "def reorder_sessions_by_endtime(df):\n",
        "    endtime = df.groupby('sessionId', sort=False).timestamp.max()\n",
        "    df_endtime = endtime.sort_values().reset_index()\n",
        "    oid2nid = dict(zip(df_endtime.sessionId, df_endtime.index))\n",
        "    sessionId_new = df.sessionId.map(oid2nid)\n",
        "    df = df.assign(sessionId=sessionId_new)\n",
        "    df = df.sort_values(['sessionId', 'timestamp'])\n",
        "    return df\n",
        "\n",
        "\n",
        "def keep_top_n_items(df, n):\n",
        "    item_support = df.groupby('itemId', sort=False).size()\n",
        "    top_items = item_support.nlargest(n).index\n",
        "    df_top = df[df.itemId.isin(top_items)]\n",
        "    return df_top\n",
        "\n",
        "\n",
        "def split_by_time(df, timedelta):\n",
        "    max_time = df.timestamp.max()\n",
        "    end_time = df.groupby('sessionId').timestamp.max()\n",
        "    split_time = max_time - timedelta\n",
        "    train_sids = end_time[end_time < split_time].index\n",
        "    df_train = df[df.sessionId.isin(train_sids)]\n",
        "    df_test = df[~df.sessionId.isin(train_sids)]\n",
        "    return df_train, df_test\n",
        "\n",
        "\n",
        "def train_test_split(df, test_split=0.2):\n",
        "    endtime = df.groupby('sessionId', sort=False).timestamp.max()\n",
        "    endtime = endtime.sort_values()\n",
        "    num_tests = int(len(endtime) * test_split)\n",
        "    test_session_ids = endtime.index[-num_tests:]\n",
        "    df_train = df[~df.sessionId.isin(test_session_ids)]\n",
        "    df_test = df[df.sessionId.isin(test_session_ids)]\n",
        "    return df_train, df_test\n",
        "\n",
        "\n",
        "def save_sessions(df, filepath):\n",
        "    df = reorder_sessions_by_endtime(df)\n",
        "    sessions = df.groupby('sessionId').itemId.apply(lambda x: ','.join(map(str, x)))\n",
        "    sessions.to_csv(filepath, sep='\\t', header=False, index=False)\n",
        "\n",
        "\n",
        "def save_dataset(dataset_dir, df_train, df_test):\n",
        "    # filter items in test but not in train\n",
        "    df_test = df_test[df_test.itemId.isin(df_train.itemId.unique())]\n",
        "    df_test = filter_short_sessions(df_test)\n",
        "\n",
        "    print(f'No. of Clicks: {len(df_train) + len(df_test)}')\n",
        "    print(f'No. of Items: {df_train.itemId.nunique()}')\n",
        "\n",
        "    # update itemId\n",
        "    train_itemId_new, uniques = pd.factorize(df_train.itemId)\n",
        "    df_train = df_train.assign(itemId=train_itemId_new)\n",
        "    oid2nid = {oid: i for i, oid in enumerate(uniques)}\n",
        "    test_itemId_new = df_test.itemId.map(oid2nid)\n",
        "    df_test = df_test.assign(itemId=test_itemId_new)\n",
        "\n",
        "    print(f'saving dataset to {dataset_dir}')\n",
        "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
        "    save_sessions(df_train, dataset_dir / 'train.txt')\n",
        "    save_sessions(df_test, dataset_dir / 'test.txt')\n",
        "    num_items = len(uniques)\n",
        "    with open(dataset_dir / 'num_items.txt', 'w') as f:\n",
        "        f.write(str(num_items))\n",
        "\n",
        "\n",
        "def preprocess_diginetica(dataset_dir, csv_file):\n",
        "    print(f'reading {csv_file}...')\n",
        "    df = pd.read_csv(\n",
        "        csv_file,\n",
        "        usecols=[0, 2, 3, 4],\n",
        "        delimiter=';',\n",
        "        parse_dates=['eventdate'],\n",
        "        infer_datetime_format=True,\n",
        "    )\n",
        "    print('start preprocessing')\n",
        "    # timeframe (time since the first query in a session, in milliseconds)\n",
        "    df['timestamp'] = pd.to_timedelta(df.timeframe, unit='ms') + df.eventdate\n",
        "    df = df.drop(['eventdate', 'timeframe'], 1)\n",
        "    df = df.sort_values(['sessionId', 'timestamp'])\n",
        "    df = filter_short_sessions(df)\n",
        "    df = truncate_long_sessions(df, is_sorted=True)\n",
        "    df = filter_infreq_items(df)\n",
        "    df = filter_short_sessions(df)\n",
        "    df_train, df_test = split_by_time(df, pd.Timedelta(days=7))\n",
        "    save_dataset(dataset_dir, df_train, df_test)\n",
        "\n",
        "\n",
        "def preprocess_gowalla_lastfm(dataset_dir, csv_file, usecols, interval, n):\n",
        "    print(f'reading {csv_file}...')\n",
        "    df = pd.read_csv(\n",
        "        csv_file,\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['userId', 'timestamp', 'itemId'],\n",
        "        usecols=usecols,\n",
        "        parse_dates=['timestamp'],\n",
        "        infer_datetime_format=True,\n",
        "    )\n",
        "    print('start preprocessing')\n",
        "    df = df.dropna()\n",
        "    df = update_id(df, 'userId')\n",
        "    df = update_id(df, 'itemId')\n",
        "    df = df.sort_values(['userId', 'timestamp'])\n",
        "\n",
        "    df = group_sessions(df, interval)\n",
        "    df = remove_immediate_repeats(df)\n",
        "    df = truncate_long_sessions(df, is_sorted=True)\n",
        "    df = keep_top_n_items(df, n)\n",
        "    df = filter_until_all_long_and_freq(df)\n",
        "    df_train, df_test = train_test_split(df, test_split=0.2)\n",
        "    save_dataset(dataset_dir, df_train, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_diginetica('./', 'train-item-views.csv')"
      ],
      "metadata": {
        "id": "5qtl7jL7qYOv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}