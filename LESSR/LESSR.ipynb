{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LESSR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCpZit6HoLA3J/ddZkHd+4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelCalabriaC/Recommender-Systems/blob/main/LESSR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ18Kd5F3uKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44afb279-1153-4509-c2cd-778c3ca377f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.1\n",
            "Collecting dgl-cu111\n",
            "  Downloading dgl_cu111-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (41.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.0 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.6.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2021.10.8)\n",
            "Installing collected packages: dgl-cu111\n",
            "Successfully installed dgl-cu111-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl\n",
        "!pip install dgl-cu111"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPU0djbi6Ddm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1b9965-ab66-4962-b5d8-682867dbce2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch as th\n",
        "from torch import nn\n",
        "import dgl\n",
        "import dgl.ops as F\n",
        "import dgl.function as fn\n",
        "\n",
        "\n",
        "class EOPA(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_dim, output_dim, batch_norm=True, feat_drop=0.0, activation=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.gru = nn.GRU(input_dim, input_dim, batch_first=True)\n",
        "        self.fc_self = nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.fc_neigh = nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.activation = activation\n",
        "\n",
        "    def reducer(self, nodes):\n",
        "        m = nodes.mailbox['m']  # (num_nodes, deg, d)\n",
        "        # m[i]: the messages passed to the i-th node with in-degree equal to 'deg'\n",
        "        # the order of messages follows the order of incoming edges\n",
        "        # since the edges are sorted by occurrence time when the EOP multigraph is built\n",
        "        # the messages are in the order required by EOPA\n",
        "        _, hn = self.gru(m)  # hn: (1, num_nodes, d)\n",
        "        return {'neigh': hn.squeeze(0)}\n",
        "\n",
        "    def forward(self, mg, feat):\n",
        "        with mg.local_scope():\n",
        "            if self.batch_norm is not None:\n",
        "                feat = self.batch_norm(feat)\n",
        "            mg.ndata['ft'] = self.feat_drop(feat)\n",
        "            if mg.number_of_edges() > 0:\n",
        "                mg.update_all(fn.copy_u('ft', 'm'), self.reducer)\n",
        "                neigh = mg.ndata['neigh']\n",
        "                rst = self.fc_self(feat) + self.fc_neigh(neigh)\n",
        "            else:\n",
        "                rst = self.fc_self(feat)\n",
        "            if self.activation is not None:\n",
        "                rst = self.activation(rst)\n",
        "            return rst\n",
        "\n",
        "\n",
        "class SGAT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        hidden_dim,\n",
        "        output_dim,\n",
        "        batch_norm=True,\n",
        "        feat_drop=0.0,\n",
        "        activation=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.fc_q = nn.Linear(input_dim, hidden_dim, bias=True)\n",
        "        self.fc_k = nn.Linear(input_dim, hidden_dim, bias=False)\n",
        "        self.fc_v = nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.fc_e = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, sg, feat):\n",
        "        if self.batch_norm is not None:\n",
        "            feat = self.batch_norm(feat)\n",
        "        feat = self.feat_drop(feat)\n",
        "        q = self.fc_q(feat)\n",
        "        k = self.fc_k(feat)\n",
        "        v = self.fc_v(feat)\n",
        "        e = F.u_add_v(sg, q, k)\n",
        "        e = self.fc_e(th.sigmoid(e))\n",
        "        a = F.edge_softmax(sg, e)\n",
        "        rst = F.u_mul_e_sum(sg, v, a)\n",
        "        if self.activation is not None:\n",
        "            rst = self.activation(rst)\n",
        "        return rst\n",
        "\n",
        "\n",
        "class AttnReadout(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        hidden_dim,\n",
        "        output_dim,\n",
        "        batch_norm=True,\n",
        "        feat_drop=0.0,\n",
        "        activation=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.fc_u = nn.Linear(input_dim, hidden_dim, bias=False)\n",
        "        self.fc_v = nn.Linear(input_dim, hidden_dim, bias=True)\n",
        "        self.fc_e = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        self.fc_out = (\n",
        "            nn.Linear(input_dim, output_dim, bias=False)\n",
        "            if output_dim != input_dim else None\n",
        "        )\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, g, feat, last_nodes):\n",
        "        if self.batch_norm is not None:\n",
        "            feat = self.batch_norm(feat)\n",
        "        feat = self.feat_drop(feat)\n",
        "        feat_u = self.fc_u(feat)\n",
        "        feat_v = self.fc_v(feat[last_nodes])\n",
        "        feat_v = dgl.broadcast_nodes(g, feat_v)\n",
        "        e = self.fc_e(th.sigmoid(feat_u + feat_v))\n",
        "        alpha = F.segment.segment_softmax(g.batch_num_nodes(), e)\n",
        "        feat_norm = feat * alpha\n",
        "        rst = F.segment.segment_reduce(g.batch_num_nodes(), feat_norm, 'sum')\n",
        "        if self.fc_out is not None:\n",
        "            rst = self.fc_out(rst)\n",
        "        if self.activation is not None:\n",
        "            rst = self.activation(rst)\n",
        "        return rst\n",
        "\n",
        "\n",
        "class LESSR(nn.Module):\n",
        "    def __init__(\n",
        "        self, num_items, embedding_dim, num_layers, batch_norm=True, feat_drop=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_items, embedding_dim, max_norm=1)\n",
        "        self.indices = nn.Parameter(\n",
        "            th.arange(num_items, dtype=th.long), requires_grad=False\n",
        "        )\n",
        "        self.num_layers = num_layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        input_dim = embedding_dim\n",
        "        for i in range(num_layers):\n",
        "            if i % 2 == 0:\n",
        "                layer = EOPA(\n",
        "                    input_dim,\n",
        "                    embedding_dim,\n",
        "                    batch_norm=batch_norm,\n",
        "                    feat_drop=feat_drop,\n",
        "                    activation=nn.PReLU(embedding_dim),\n",
        "                )\n",
        "            else:\n",
        "                layer = SGAT(\n",
        "                    input_dim,\n",
        "                    embedding_dim,\n",
        "                    embedding_dim,\n",
        "                    batch_norm=batch_norm,\n",
        "                    feat_drop=feat_drop,\n",
        "                    activation=nn.PReLU(embedding_dim),\n",
        "                )\n",
        "            input_dim += embedding_dim\n",
        "            self.layers.append(layer)\n",
        "        self.readout = AttnReadout(\n",
        "            input_dim,\n",
        "            embedding_dim,\n",
        "            embedding_dim,\n",
        "            batch_norm=batch_norm,\n",
        "            feat_drop=feat_drop,\n",
        "            activation=nn.PReLU(embedding_dim),\n",
        "        )\n",
        "        input_dim += embedding_dim\n",
        "        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.fc_sr = nn.Linear(input_dim, embedding_dim, bias=False)\n",
        "\n",
        "    def forward(self, mg, sg=None):\n",
        "        iid = mg.ndata['iid']\n",
        "        feat = self.embedding(iid)\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i % 2 == 0:\n",
        "                out = layer(mg, feat)\n",
        "            else:\n",
        "                out = layer(sg, feat)\n",
        "            feat = th.cat([out, feat], dim=1)\n",
        "        last_nodes = mg.filter_nodes(lambda nodes: nodes.data['last'] == 1)\n",
        "        sr_g = self.readout(mg, feat, last_nodes)\n",
        "        sr_l = feat[last_nodes]\n",
        "        sr = th.cat([sr_l, sr_g], dim=1)\n",
        "        if self.batch_norm is not None:\n",
        "            sr = self.batch_norm(sr)\n",
        "        sr = self.fc_sr(self.feat_drop(sr))\n",
        "        logits = sr @ self.embedding(self.indices).t()\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch as th\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "# ignore weight decay for parameters in bias, batch norm and activation\n",
        "def fix_weight_decay(model):\n",
        "    decay = []\n",
        "    no_decay = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if any(map(lambda x: x in name, ['bias', 'batch_norm', 'activation'])):\n",
        "            no_decay.append(param)\n",
        "        else:\n",
        "            decay.append(param)\n",
        "    params = [{'params': decay}, {'params': no_decay, 'weight_decay': 0}]\n",
        "    return params\n",
        "\n",
        "\n",
        "def prepare_batch(batch, device):\n",
        "    inputs, labels = batch\n",
        "    inputs_gpu = [x.to(device) for x in inputs]\n",
        "    labels_gpu = labels.to(device)\n",
        "    return inputs_gpu, labels_gpu\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, device, Ks=[20]):\n",
        "    model.eval()\n",
        "    num_samples = 0\n",
        "    max_K = max(Ks)\n",
        "    results = defaultdict(float)\n",
        "    with th.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputs, labels = prepare_batch(batch, device)\n",
        "            logits = model(*inputs)\n",
        "            batch_size = logits.size(0)\n",
        "            num_samples += batch_size\n",
        "            topk = th.topk(logits, k=max_K, sorted=True)[1]\n",
        "            labels = labels.unsqueeze(-1)\n",
        "            for K in Ks:\n",
        "                hit_ranks = th.where(topk[:, :K] == labels)[1] + 1\n",
        "                hit_ranks = hit_ranks.float().cpu()\n",
        "                results[f'HR@{K}'] += hit_ranks.numel()\n",
        "                results[f'MRR@{K}'] += hit_ranks.reciprocal().sum().item()\n",
        "                results[f'NDCG@{K}'] += th.log2(1 + hit_ranks).reciprocal().sum().item()\n",
        "    for metric in results:\n",
        "        results[metric] /= num_samples\n",
        "    return results\n",
        "\n",
        "\n",
        "def print_results(results, epochs=None):\n",
        "    print('Metric\\t' + '\\t'.join(results.keys()))\n",
        "    print(\n",
        "        'Value\\t' +\n",
        "        '\\t'.join([f'{round(val * 100, 2):.2f}' for val in results.values()])\n",
        "    )\n",
        "    if epochs is not None:\n",
        "        print('Epoch\\t' + '\\t'.join([str(epochs[metric]) for metric in results]))\n",
        "\n",
        "\n",
        "class TrainRunner:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        train_loader,\n",
        "        test_loader,\n",
        "        device,\n",
        "        lr=1e-3,\n",
        "        weight_decay=0,\n",
        "        patience=3,\n",
        "        Ks=[20],\n",
        "    ):\n",
        "        self.model = model\n",
        "        if weight_decay > 0:\n",
        "            params = fix_weight_decay(model)\n",
        "        else:\n",
        "            params = model.parameters()\n",
        "        self.optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.epoch = 0\n",
        "        self.batch = 0\n",
        "        self.patience = patience\n",
        "        self.Ks = Ks\n",
        "\n",
        "    def train(self, epochs, log_interval=100):\n",
        "        max_results = defaultdict(float)\n",
        "        max_epochs = defaultdict(int)\n",
        "        bad_counter = 0\n",
        "        t = time.time()\n",
        "        mean_loss = 0\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            for batch in self.train_loader:\n",
        "                inputs, labels = prepare_batch(batch, self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                logits = self.model(*inputs)\n",
        "                loss = nn.functional.cross_entropy(logits, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                mean_loss += loss.item() / log_interval\n",
        "                if self.batch > 0 and self.batch % log_interval == 0:\n",
        "                    print(\n",
        "                        f'Batch {self.batch}: Loss = {mean_loss:.4f}, Time Elapsed = {time.time() - t:.2f}s'\n",
        "                    )\n",
        "                    t = time.time()\n",
        "                    mean_loss = 0\n",
        "                self.batch += 1\n",
        "\n",
        "            curr_results = evaluate(\n",
        "                self.model, self.test_loader, self.device, Ks=self.Ks\n",
        "            )\n",
        "\n",
        "            print(f'\\nEpoch {self.epoch}:')\n",
        "            print_results(curr_results)\n",
        "\n",
        "            any_better_result = False\n",
        "            for metric in curr_results:\n",
        "                if curr_results[metric] > max_results[metric]:\n",
        "                    max_results[metric] = curr_results[metric]\n",
        "                    max_epochs[metric] = self.epoch\n",
        "                    any_better_result = True\n",
        "\n",
        "            if any_better_result:\n",
        "                bad_counter = 0\n",
        "            else:\n",
        "                bad_counter += 1\n",
        "                if bad_counter == self.patience:\n",
        "                    break\n",
        "\n",
        "            self.epoch += 1\n",
        "        print('\\nBest results')\n",
        "        print_results(max_results, max_epochs)\n",
        "        return max_results"
      ],
      "metadata": {
        "id": "xsMVCFIF6KO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import dgl\n",
        "\n",
        "\n",
        "def label_last(g, last_nid):\n",
        "    is_last = th.zeros(g.number_of_nodes(), dtype=th.int32)\n",
        "    is_last[last_nid] = 1\n",
        "    g.ndata['last'] = is_last\n",
        "    return g\n",
        "\n",
        "\n",
        "def seq_to_eop_multigraph(seq):\n",
        "    items = np.unique(seq)\n",
        "    iid2nid = {iid: i for i, iid in enumerate(items)}\n",
        "    num_nodes = len(items)\n",
        "\n",
        "    if len(seq) > 1:\n",
        "        seq_nid = [iid2nid[iid] for iid in seq]\n",
        "        src = seq_nid[:-1]\n",
        "        dst = seq_nid[1:]\n",
        "    else:\n",
        "        src = th.LongTensor([])\n",
        "        dst = th.LongTensor([])\n",
        "    g = dgl.graph((src, dst), num_nodes=num_nodes)\n",
        "    g.ndata['iid'] = th.tensor(items, dtype=th.long)\n",
        "    label_last(g, iid2nid[seq[-1]])\n",
        "    return g\n",
        "\n",
        "\n",
        "def seq_to_shortcut_graph(seq):\n",
        "    items = np.unique(seq)\n",
        "    iid2nid = {iid: i for i, iid in enumerate(items)}\n",
        "    num_nodes = len(items)\n",
        "\n",
        "    seq_nid = [iid2nid[iid] for iid in seq]\n",
        "    counter = Counter(\n",
        "        [(seq_nid[i], seq_nid[j]) for i in range(len(seq)) for j in range(i, len(seq))]\n",
        "    )\n",
        "    edges = counter.keys()\n",
        "    src, dst = zip(*edges)\n",
        "\n",
        "    g = dgl.graph((src, dst), num_nodes=num_nodes)\n",
        "    return g\n",
        "\n",
        "\n",
        "def collate_fn_factory(*seq_to_graph_fns):\n",
        "    def collate_fn(samples):\n",
        "        seqs, labels = zip(*samples)\n",
        "        inputs = []\n",
        "        for seq_to_graph in seq_to_graph_fns:\n",
        "            graphs = list(map(seq_to_graph, seqs))\n",
        "            bg = dgl.batch(graphs)\n",
        "            inputs.append(bg)\n",
        "        labels = th.LongTensor(labels)\n",
        "        return inputs, labels\n",
        "\n",
        "    return collate_fn"
      ],
      "metadata": {
        "id": "4oUdXWuJ79wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def create_index(sessions):\n",
        "    lens = np.fromiter(map(len, sessions), dtype=np.long)\n",
        "    session_idx = np.repeat(np.arange(len(sessions)), lens - 1)\n",
        "    label_idx = map(lambda l: range(1, l), lens)\n",
        "    label_idx = itertools.chain.from_iterable(label_idx)\n",
        "    label_idx = np.fromiter(label_idx, dtype=np.long)\n",
        "    idx = np.column_stack((session_idx, label_idx))\n",
        "    return idx\n",
        "\n",
        "\n",
        "def read_sessions(filepath):\n",
        "    sessions = pd.read_csv(filepath, sep='\\t', header=None, squeeze=True)\n",
        "    sessions = sessions.apply(lambda x: list(map(int, x.split(',')))).values\n",
        "    return sessions\n",
        "\n",
        "\n",
        "def read_dataset(dataset_dir):\n",
        "    train_sessions = read_sessions('train.txt')\n",
        "    test_sessions = read_sessions('test.txt')\n",
        "    with open('num_items.txt', 'r') as f:\n",
        "        num_items = int(f.readline())\n",
        "    return train_sessions, test_sessions, num_items\n",
        "\n",
        "\n",
        "class AugmentedDataset:\n",
        "    def __init__(self, sessions, sort_by_length=True):\n",
        "        self.sessions = sessions\n",
        "        index = create_index(sessions)  # columns: sessionId, labelIndex\n",
        "        if sort_by_length:\n",
        "            # sort by labelIndex in descending order\n",
        "            ind = np.argsort(index[:, 1])[::-1]\n",
        "            index = index[ind]\n",
        "        self.index = index\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sid, lidx = self.index[idx]\n",
        "        seq = self.sessions[sid][:lidx]\n",
        "        label = self.sessions[sid][lidx]\n",
        "        return seq, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)"
      ],
      "metadata": {
        "id": "2xvy4LlJ8Eim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "parser.add_argument('--dataset-dir', default='datasets/sample', help='the dataset directory')\n",
        "parser.add_argument('--embedding-dim', type=int, default=32, help='the embedding size')\n",
        "parser.add_argument('--num-layers', type=int, default=3, help='the number of layers')\n",
        "parser.add_argument('--feat-drop', type=float, default=0.2, help='the dropout ratio for features')\n",
        "parser.add_argument('--lr', type=float, default=1e-3, help='the learning rate')\n",
        "parser.add_argument('--batch-size', type=int, default=512, help='the batch size for training')\n",
        "parser.add_argument('--epochs', type=int, default=30, help='the number of training epochs')\n",
        "parser.add_argument('--weight-decay',type=float,default=1e-4,help='the parameter for L2 regularization',)\n",
        "parser.add_argument('--Ks',default='10,20',help='the values of K in evaluation metrics, separated by commas',)\n",
        "parser.add_argument('--patience',type=int,default=2,help='the number of epochs that the performance does not improves after which the training stops',)\n",
        "parser.add_argument('--num-workers',type=int,default=4,help='the number of processes to load the input graphs',)\n",
        "parser.add_argument('--valid-split',type=float,default=None,help='the fraction for the validation set',)\n",
        "parser.add_argument('--log-interval',type=int,default=100,help='print the loss after this number of iterations',)\n",
        "args = parser.parse_args()\n",
        "print(args)\"\"\"\n",
        "\n",
        "import gc\n",
        "import dgl\n",
        "from pathlib import Path\n",
        "import torch as th\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "ks = [5,10,20]\n",
        "print('reading dataset')\n",
        "train_sessions, test_sessions, num_items = read_dataset('.')\n",
        "\n",
        "train_set = AugmentedDataset(train_sessions)\n",
        "test_set = AugmentedDataset(test_sessions)\n",
        "\n",
        "if 3 > 1:\n",
        "    collate_fn = collate_fn_factory(seq_to_eop_multigraph, seq_to_shortcut_graph)\n",
        "else:\n",
        "    collate_fn = collate_fn_factory(seq_to_eop_multigraph)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_set,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "model = LESSR(num_items, 32,3, feat_drop=0.2)\n",
        "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "runner = TrainRunner(\n",
        "    model,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    device=device,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=2,\n",
        "    Ks=ks,\n",
        ")\n",
        "\n",
        "print('start training')\n",
        "runner.train(30, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMqyTush64QV",
        "outputId": "6a60ca74-198f-4f04-8dc1-d5532a411c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading dataset\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LESSR(\n",
            "  (embedding): Embedding(23107, 32, max_norm=1)\n",
            "  (layers): ModuleList(\n",
            "    (0): EOPA(\n",
            "      (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (feat_drop): Dropout(p=0.2, inplace=False)\n",
            "      (gru): GRU(32, 32, batch_first=True)\n",
            "      (fc_self): Linear(in_features=32, out_features=32, bias=False)\n",
            "      (fc_neigh): Linear(in_features=32, out_features=32, bias=False)\n",
            "      (activation): PReLU(num_parameters=32)\n",
            "    )\n",
            "    (1): SGAT(\n",
            "      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (feat_drop): Dropout(p=0.2, inplace=False)\n",
            "      (fc_q): Linear(in_features=64, out_features=32, bias=True)\n",
            "      (fc_k): Linear(in_features=64, out_features=32, bias=False)\n",
            "      (fc_v): Linear(in_features=64, out_features=32, bias=False)\n",
            "      (fc_e): Linear(in_features=32, out_features=1, bias=False)\n",
            "      (activation): PReLU(num_parameters=32)\n",
            "    )\n",
            "    (2): EOPA(\n",
            "      (batch_norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (feat_drop): Dropout(p=0.2, inplace=False)\n",
            "      (gru): GRU(96, 96, batch_first=True)\n",
            "      (fc_self): Linear(in_features=96, out_features=32, bias=False)\n",
            "      (fc_neigh): Linear(in_features=96, out_features=32, bias=False)\n",
            "      (activation): PReLU(num_parameters=32)\n",
            "    )\n",
            "  )\n",
            "  (readout): AttnReadout(\n",
            "    (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (feat_drop): Dropout(p=0.2, inplace=False)\n",
            "    (fc_u): Linear(in_features=128, out_features=32, bias=False)\n",
            "    (fc_v): Linear(in_features=128, out_features=32, bias=True)\n",
            "    (fc_e): Linear(in_features=32, out_features=1, bias=False)\n",
            "    (fc_out): Linear(in_features=128, out_features=32, bias=False)\n",
            "    (activation): PReLU(num_parameters=32)\n",
            "  )\n",
            "  (batch_norm): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (feat_drop): Dropout(p=0.2, inplace=False)\n",
            "  (fc_sr): Linear(in_features=160, out_features=32, bias=False)\n",
            ")\n",
            "start training\n",
            "Batch 100: Loss = 10.1017, Time Elapsed = 16.70s\n",
            "Batch 200: Loss = 9.7568, Time Elapsed = 13.85s\n",
            "Batch 300: Loss = 9.5524, Time Elapsed = 13.50s\n",
            "Batch 400: Loss = 9.3464, Time Elapsed = 13.68s\n",
            "Batch 500: Loss = 9.1171, Time Elapsed = 13.94s\n",
            "Batch 600: Loss = 8.9132, Time Elapsed = 14.78s\n",
            "Batch 700: Loss = 8.7217, Time Elapsed = 14.20s\n",
            "Batch 800: Loss = 8.5089, Time Elapsed = 13.80s\n",
            "Batch 900: Loss = 8.3311, Time Elapsed = 13.80s\n",
            "Batch 1000: Loss = 8.1412, Time Elapsed = 13.82s\n",
            "Batch 1100: Loss = 7.9270, Time Elapsed = 13.79s\n",
            "Batch 1200: Loss = 7.7566, Time Elapsed = 13.70s\n",
            "Batch 1300: Loss = 7.6303, Time Elapsed = 13.76s\n",
            "Batch 1400: Loss = 7.4923, Time Elapsed = 13.82s\n",
            "\n",
            "Epoch 0:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t20.74\t14.58\t16.12\t25.05\t15.16\t17.51\t29.24\t15.45\t18.57\n",
            "Batch 1500: Loss = 7.1225, Time Elapsed = 56.54s\n",
            "Batch 1600: Loss = 6.8416, Time Elapsed = 14.02s\n",
            "Batch 1700: Loss = 6.6684, Time Elapsed = 14.07s\n",
            "Batch 1800: Loss = 6.5746, Time Elapsed = 14.08s\n",
            "Batch 1900: Loss = 6.5027, Time Elapsed = 14.09s\n",
            "Batch 2000: Loss = 6.4087, Time Elapsed = 14.14s\n",
            "Batch 2100: Loss = 6.3127, Time Elapsed = 14.04s\n",
            "Batch 2200: Loss = 6.2064, Time Elapsed = 14.14s\n",
            "Batch 2300: Loss = 6.1558, Time Elapsed = 13.97s\n",
            "Batch 2400: Loss = 6.0689, Time Elapsed = 14.00s\n",
            "Batch 2500: Loss = 6.0238, Time Elapsed = 14.20s\n",
            "Batch 2600: Loss = 5.9308, Time Elapsed = 13.91s\n",
            "Batch 2700: Loss = 5.8740, Time Elapsed = 13.63s\n",
            "Batch 2800: Loss = 5.7935, Time Elapsed = 13.71s\n",
            "Batch 2900: Loss = 5.7196, Time Elapsed = 13.78s\n",
            "\n",
            "Epoch 1:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t28.30\t18.15\t20.67\t35.82\t19.15\t23.10\t43.14\t19.66\t24.95\n",
            "Batch 3000: Loss = 5.3528, Time Elapsed = 56.02s\n",
            "Batch 3100: Loss = 5.3472, Time Elapsed = 14.55s\n",
            "Batch 3200: Loss = 5.3353, Time Elapsed = 14.18s\n",
            "Batch 3300: Loss = 5.3241, Time Elapsed = 14.11s\n",
            "Batch 3400: Loss = 5.3285, Time Elapsed = 14.08s\n",
            "Batch 3500: Loss = 5.2911, Time Elapsed = 14.14s\n",
            "Batch 3600: Loss = 5.2786, Time Elapsed = 14.22s\n",
            "Batch 3700: Loss = 5.2195, Time Elapsed = 14.01s\n",
            "Batch 3800: Loss = 5.2731, Time Elapsed = 14.42s\n",
            "Batch 3900: Loss = 5.2732, Time Elapsed = 14.11s\n",
            "Batch 4000: Loss = 5.1702, Time Elapsed = 13.95s\n",
            "Batch 4100: Loss = 5.1679, Time Elapsed = 14.03s\n",
            "Batch 4200: Loss = 5.0962, Time Elapsed = 13.97s\n",
            "Batch 4300: Loss = 5.1488, Time Elapsed = 14.07s\n",
            "\n",
            "Epoch 2:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t31.24\t19.79\t22.63\t40.12\t20.98\t25.51\t48.86\t21.59\t27.72\n",
            "Batch 4400: Loss = 4.9808, Time Elapsed = 57.73s\n",
            "Batch 4500: Loss = 4.8251, Time Elapsed = 13.93s\n",
            "Batch 4600: Loss = 4.8396, Time Elapsed = 14.03s\n",
            "Batch 4700: Loss = 4.8818, Time Elapsed = 14.36s\n",
            "Batch 4800: Loss = 4.8387, Time Elapsed = 14.10s\n",
            "Batch 4900: Loss = 4.8967, Time Elapsed = 13.83s\n",
            "Batch 5000: Loss = 4.8694, Time Elapsed = 13.83s\n",
            "Batch 5100: Loss = 4.8628, Time Elapsed = 13.82s\n",
            "Batch 5200: Loss = 4.8845, Time Elapsed = 13.91s\n",
            "Batch 5300: Loss = 4.8650, Time Elapsed = 13.83s\n",
            "Batch 5400: Loss = 4.8315, Time Elapsed = 13.90s\n",
            "Batch 5500: Loss = 4.8547, Time Elapsed = 13.92s\n",
            "Batch 5600: Loss = 4.8435, Time Elapsed = 13.80s\n",
            "Batch 5700: Loss = 4.8236, Time Elapsed = 13.82s\n",
            "Batch 5800: Loss = 4.8426, Time Elapsed = 13.88s\n",
            "\n",
            "Epoch 3:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t32.45\t20.66\t23.58\t41.99\t21.94\t26.67\t51.33\t22.58\t29.03\n",
            "Batch 5900: Loss = 4.5743, Time Elapsed = 55.85s\n",
            "Batch 6000: Loss = 4.5734, Time Elapsed = 13.83s\n",
            "Batch 6100: Loss = 4.5779, Time Elapsed = 13.71s\n",
            "Batch 6200: Loss = 4.6103, Time Elapsed = 13.61s\n",
            "Batch 6300: Loss = 4.6293, Time Elapsed = 13.75s\n",
            "Batch 6400: Loss = 4.6099, Time Elapsed = 13.74s\n",
            "Batch 6500: Loss = 4.6456, Time Elapsed = 13.59s\n",
            "Batch 6600: Loss = 4.6330, Time Elapsed = 13.83s\n",
            "Batch 6700: Loss = 4.7064, Time Elapsed = 13.71s\n",
            "Batch 6800: Loss = 4.6810, Time Elapsed = 13.87s\n",
            "Batch 6900: Loss = 4.6498, Time Elapsed = 13.86s\n",
            "Batch 7000: Loss = 4.6706, Time Elapsed = 13.68s\n",
            "Batch 7100: Loss = 4.6702, Time Elapsed = 13.77s\n",
            "Batch 7200: Loss = 4.6775, Time Elapsed = 13.69s\n",
            "\n",
            "Epoch 4:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t33.25\t21.26\t24.23\t43.08\t22.57\t27.41\t52.56\t23.23\t29.81\n",
            "Batch 7300: Loss = 4.5593, Time Elapsed = 55.98s\n",
            "Batch 7400: Loss = 4.3401, Time Elapsed = 13.86s\n",
            "Batch 7500: Loss = 4.4144, Time Elapsed = 14.15s\n",
            "Batch 7600: Loss = 4.4546, Time Elapsed = 14.05s\n",
            "Batch 7700: Loss = 4.4621, Time Elapsed = 13.91s\n",
            "Batch 7800: Loss = 4.4780, Time Elapsed = 14.06s\n",
            "Batch 7900: Loss = 4.5248, Time Elapsed = 13.92s\n",
            "Batch 8000: Loss = 4.5416, Time Elapsed = 13.92s\n",
            "Batch 8100: Loss = 4.4805, Time Elapsed = 13.79s\n",
            "Batch 8200: Loss = 4.5045, Time Elapsed = 14.16s\n",
            "Batch 8300: Loss = 4.5426, Time Elapsed = 14.10s\n",
            "Batch 8400: Loss = 4.5407, Time Elapsed = 14.06s\n",
            "Batch 8500: Loss = 4.5627, Time Elapsed = 13.82s\n",
            "Batch 8600: Loss = 4.5292, Time Elapsed = 13.70s\n",
            "Batch 8700: Loss = 4.5543, Time Elapsed = 13.61s\n",
            "\n",
            "Epoch 5:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t33.86\t21.70\t24.71\t43.79\t23.03\t27.93\t53.49\t23.70\t30.38\n",
            "Batch 8800: Loss = 4.3396, Time Elapsed = 56.74s\n",
            "Batch 8900: Loss = 4.2780, Time Elapsed = 13.77s\n",
            "Batch 9000: Loss = 4.3374, Time Elapsed = 13.72s\n",
            "Batch 9100: Loss = 4.3756, Time Elapsed = 13.74s\n",
            "Batch 9200: Loss = 4.3338, Time Elapsed = 13.87s\n",
            "Batch 9300: Loss = 4.3860, Time Elapsed = 13.71s\n",
            "Batch 9400: Loss = 4.3887, Time Elapsed = 13.90s\n",
            "Batch 9500: Loss = 4.4077, Time Elapsed = 13.66s\n",
            "Batch 9600: Loss = 4.4075, Time Elapsed = 13.71s\n",
            "Batch 9700: Loss = 4.4056, Time Elapsed = 14.04s\n",
            "Batch 9800: Loss = 4.4128, Time Elapsed = 13.81s\n",
            "Batch 9900: Loss = 4.4224, Time Elapsed = 13.75s\n",
            "Batch 10000: Loss = 4.4582, Time Elapsed = 13.61s\n",
            "Batch 10100: Loss = 4.4679, Time Elapsed = 13.67s\n",
            "\n",
            "Epoch 6:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t34.04\t21.85\t24.87\t43.95\t23.18\t28.08\t53.88\t23.87\t30.60\n",
            "Batch 10200: Loss = 4.3802, Time Elapsed = 55.38s\n",
            "Batch 10300: Loss = 4.1839, Time Elapsed = 13.54s\n",
            "Batch 10400: Loss = 4.2036, Time Elapsed = 13.77s\n",
            "Batch 10500: Loss = 4.2261, Time Elapsed = 13.71s\n",
            "Batch 10600: Loss = 4.2486, Time Elapsed = 13.55s\n",
            "Batch 10700: Loss = 4.2765, Time Elapsed = 13.77s\n",
            "Batch 10800: Loss = 4.3043, Time Elapsed = 13.60s\n",
            "Batch 10900: Loss = 4.3086, Time Elapsed = 13.54s\n",
            "Batch 11000: Loss = 4.3224, Time Elapsed = 13.66s\n",
            "Batch 11100: Loss = 4.3263, Time Elapsed = 13.76s\n",
            "Batch 11200: Loss = 4.3708, Time Elapsed = 13.68s\n",
            "Batch 11300: Loss = 4.3608, Time Elapsed = 13.64s\n",
            "Batch 11400: Loss = 4.3642, Time Elapsed = 13.64s\n",
            "Batch 11500: Loss = 4.3768, Time Elapsed = 13.62s\n",
            "Batch 11600: Loss = 4.3860, Time Elapsed = 13.56s\n",
            "\n",
            "Epoch 7:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t34.20\t21.98\t25.01\t44.13\t23.32\t28.23\t54.29\t24.02\t30.80\n",
            "Batch 11700: Loss = 4.1708, Time Elapsed = 54.89s\n",
            "Batch 11800: Loss = 4.0980, Time Elapsed = 13.54s\n",
            "Batch 11900: Loss = 4.1523, Time Elapsed = 13.64s\n",
            "Batch 12000: Loss = 4.1857, Time Elapsed = 13.90s\n",
            "Batch 12100: Loss = 4.1911, Time Elapsed = 13.80s\n",
            "Batch 12200: Loss = 4.2094, Time Elapsed = 13.57s\n",
            "Batch 12300: Loss = 4.2559, Time Elapsed = 13.87s\n",
            "Batch 12400: Loss = 4.2552, Time Elapsed = 13.90s\n",
            "Batch 12500: Loss = 4.2923, Time Elapsed = 13.98s\n",
            "Batch 12600: Loss = 4.2909, Time Elapsed = 13.68s\n",
            "Batch 12700: Loss = 4.2678, Time Elapsed = 13.70s\n",
            "Batch 12800: Loss = 4.3052, Time Elapsed = 13.73s\n",
            "Batch 12900: Loss = 4.2957, Time Elapsed = 13.69s\n",
            "Batch 13000: Loss = 4.3155, Time Elapsed = 13.74s\n",
            "\n",
            "Epoch 8:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t34.46\t22.17\t25.22\t44.41\t23.50\t28.44\t54.32\t24.19\t30.95\n",
            "Batch 13100: Loss = 4.2692, Time Elapsed = 54.96s\n",
            "Batch 13200: Loss = 4.0363, Time Elapsed = 13.56s\n",
            "Batch 13300: Loss = 4.0776, Time Elapsed = 13.59s\n",
            "Batch 13400: Loss = 4.1044, Time Elapsed = 13.48s\n",
            "Batch 13500: Loss = 4.1343, Time Elapsed = 13.93s\n",
            "Batch 13600: Loss = 4.1180, Time Elapsed = 14.05s\n",
            "Batch 13700: Loss = 4.1801, Time Elapsed = 13.71s\n",
            "Batch 13800: Loss = 4.1962, Time Elapsed = 13.44s\n",
            "Batch 13900: Loss = 4.1840, Time Elapsed = 13.64s\n",
            "Batch 14000: Loss = 4.2331, Time Elapsed = 13.58s\n",
            "Batch 14100: Loss = 4.2048, Time Elapsed = 13.56s\n",
            "Batch 14200: Loss = 4.2365, Time Elapsed = 13.41s\n",
            "Batch 14300: Loss = 4.2366, Time Elapsed = 13.59s\n",
            "Batch 14400: Loss = 4.2618, Time Elapsed = 13.55s\n",
            "Batch 14500: Loss = 4.2529, Time Elapsed = 13.67s\n",
            "\n",
            "Epoch 9:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t34.49\t22.24\t25.28\t44.29\t23.55\t28.45\t54.36\t24.25\t31.00\n",
            "Batch 14600: Loss = 4.0707, Time Elapsed = 54.93s\n",
            "Batch 14700: Loss = 3.9767, Time Elapsed = 13.64s\n",
            "Batch 14800: Loss = 4.0072, Time Elapsed = 13.71s\n",
            "Batch 14900: Loss = 4.0638, Time Elapsed = 13.68s\n",
            "Batch 15000: Loss = 4.0768, Time Elapsed = 13.92s\n",
            "Batch 15100: Loss = 4.1029, Time Elapsed = 13.44s\n",
            "Batch 15200: Loss = 4.1161, Time Elapsed = 13.60s\n",
            "Batch 15300: Loss = 4.1641, Time Elapsed = 13.58s\n",
            "Batch 15400: Loss = 4.1630, Time Elapsed = 13.55s\n",
            "Batch 15500: Loss = 4.1409, Time Elapsed = 13.53s\n",
            "Batch 15600: Loss = 4.2194, Time Elapsed = 13.62s\n",
            "Batch 15700: Loss = 4.1647, Time Elapsed = 13.94s\n",
            "Batch 15800: Loss = 4.2450, Time Elapsed = 13.88s\n",
            "Batch 15900: Loss = 4.2178, Time Elapsed = 14.08s\n",
            "\n",
            "Epoch 10:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t34.43\t22.42\t25.40\t44.46\t23.76\t28.65\t54.47\t24.46\t31.18\n",
            "Batch 16000: Loss = 4.1948, Time Elapsed = 54.62s\n",
            "Batch 16100: Loss = 3.9183, Time Elapsed = 13.68s\n",
            "Batch 16200: Loss = 3.9540, Time Elapsed = 13.50s\n",
            "Batch 16300: Loss = 3.9879, Time Elapsed = 13.61s\n",
            "Batch 16400: Loss = 4.0147, Time Elapsed = 13.64s\n",
            "Batch 16500: Loss = 4.0302, Time Elapsed = 13.46s\n",
            "Batch 16600: Loss = 4.0640, Time Elapsed = 13.54s\n",
            "Batch 16700: Loss = 4.0943, Time Elapsed = 13.43s\n",
            "Batch 16800: Loss = 4.1111, Time Elapsed = 13.46s\n",
            "Batch 16900: Loss = 4.1171, Time Elapsed = 13.51s\n",
            "Batch 17000: Loss = 4.1604, Time Elapsed = 13.60s\n",
            "Batch 17100: Loss = 4.1548, Time Elapsed = 13.60s\n",
            "Batch 17200: Loss = 4.1777, Time Elapsed = 13.37s\n",
            "Batch 17300: Loss = 4.1710, Time Elapsed = 13.38s\n",
            "Batch 17400: Loss = 4.1641, Time Elapsed = 13.69s\n",
            "\n",
            "Epoch 11:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t34.52\t22.42\t25.42\t44.44\t23.74\t28.62\t54.25\t24.42\t31.11\n",
            "Batch 17500: Loss = 4.0258, Time Elapsed = 54.44s\n",
            "Batch 17600: Loss = 3.8675, Time Elapsed = 13.51s\n",
            "Batch 17700: Loss = 3.9219, Time Elapsed = 13.52s\n",
            "Batch 17800: Loss = 3.9776, Time Elapsed = 13.65s\n",
            "Batch 17900: Loss = 3.9927, Time Elapsed = 13.39s\n",
            "Batch 18000: Loss = 4.0000, Time Elapsed = 13.64s\n",
            "Batch 18100: Loss = 4.0432, Time Elapsed = 13.72s\n",
            "Batch 18200: Loss = 4.0543, Time Elapsed = 13.57s\n",
            "Batch 18300: Loss = 4.1052, Time Elapsed = 13.33s\n",
            "Batch 18400: Loss = 4.0807, Time Elapsed = 13.61s\n",
            "Batch 18500: Loss = 4.1232, Time Elapsed = 13.53s\n",
            "Batch 18600: Loss = 4.0913, Time Elapsed = 13.47s\n",
            "Batch 18700: Loss = 4.1455, Time Elapsed = 13.62s\n",
            "Batch 18800: Loss = 4.1508, Time Elapsed = 13.53s\n",
            "Batch 18900: Loss = 4.1573, Time Elapsed = 13.52s\n",
            "\n",
            "Epoch 12:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t34.58\t22.26\t25.31\t44.44\t23.57\t28.50\t54.30\t24.26\t30.99\n",
            "Batch 19000: Loss = 3.7992, Time Elapsed = 54.74s\n",
            "Batch 19100: Loss = 3.8590, Time Elapsed = 13.46s\n",
            "Batch 19200: Loss = 3.9285, Time Elapsed = 13.56s\n",
            "Batch 19300: Loss = 3.9503, Time Elapsed = 13.46s\n",
            "Batch 19400: Loss = 3.9781, Time Elapsed = 13.57s\n",
            "Batch 19500: Loss = 3.9930, Time Elapsed = 13.67s\n",
            "Batch 19600: Loss = 4.0242, Time Elapsed = 13.51s\n",
            "Batch 19700: Loss = 4.0336, Time Elapsed = 13.59s\n",
            "Batch 19800: Loss = 4.0450, Time Elapsed = 13.77s\n",
            "Batch 19900: Loss = 4.0492, Time Elapsed = 14.00s\n",
            "Batch 20000: Loss = 4.1036, Time Elapsed = 13.81s\n",
            "Batch 20100: Loss = 4.0977, Time Elapsed = 13.47s\n",
            "Batch 20200: Loss = 4.1150, Time Elapsed = 13.61s\n",
            "Batch 20300: Loss = 4.1262, Time Elapsed = 13.48s\n",
            "\n",
            "Epoch 13:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t34.71\t22.27\t25.35\t44.38\t23.56\t28.48\t54.15\t24.24\t30.96\n",
            "Batch 20400: Loss = 3.9519, Time Elapsed = 54.85s\n",
            "Batch 20500: Loss = 3.8368, Time Elapsed = 13.76s\n",
            "Batch 20600: Loss = 3.8466, Time Elapsed = 13.65s\n",
            "Batch 20700: Loss = 3.9123, Time Elapsed = 13.41s\n",
            "Batch 20800: Loss = 3.9230, Time Elapsed = 13.52s\n",
            "Batch 20900: Loss = 3.9469, Time Elapsed = 13.50s\n",
            "Batch 21000: Loss = 3.9880, Time Elapsed = 13.58s\n",
            "Batch 21100: Loss = 4.0110, Time Elapsed = 13.55s\n",
            "Batch 21200: Loss = 4.0054, Time Elapsed = 13.37s\n",
            "Batch 21300: Loss = 4.0389, Time Elapsed = 13.61s\n",
            "Batch 21400: Loss = 4.0413, Time Elapsed = 13.66s\n",
            "Batch 21500: Loss = 4.0628, Time Elapsed = 13.81s\n",
            "Batch 21600: Loss = 4.0534, Time Elapsed = 13.51s\n",
            "Batch 21700: Loss = 4.0599, Time Elapsed = 13.49s\n",
            "Batch 21800: Loss = 4.1060, Time Elapsed = 13.54s\n",
            "\n",
            "Epoch 14:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t34.41\t22.23\t25.25\t44.42\t23.57\t28.49\t54.00\t24.24\t30.92\n",
            "Batch 21900: Loss = 3.7738, Time Elapsed = 56.66s\n",
            "Batch 22000: Loss = 3.7910, Time Elapsed = 13.66s\n",
            "Batch 22100: Loss = 3.8260, Time Elapsed = 13.91s\n",
            "Batch 22200: Loss = 3.8926, Time Elapsed = 13.49s\n",
            "Batch 22300: Loss = 3.9238, Time Elapsed = 13.54s\n",
            "Batch 22400: Loss = 3.9427, Time Elapsed = 13.69s\n",
            "Batch 22500: Loss = 3.9504, Time Elapsed = 13.74s\n",
            "Batch 22600: Loss = 3.9897, Time Elapsed = 13.45s\n",
            "Batch 22700: Loss = 3.9932, Time Elapsed = 13.44s\n",
            "Batch 22800: Loss = 3.9891, Time Elapsed = 13.76s\n",
            "Batch 22900: Loss = 4.0394, Time Elapsed = 13.61s\n",
            "Batch 23000: Loss = 4.0270, Time Elapsed = 13.56s\n",
            "Batch 23100: Loss = 4.0723, Time Elapsed = 13.75s\n",
            "Batch 23200: Loss = 4.0694, Time Elapsed = 13.54s\n",
            "\n",
            "Epoch 15:\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t34.44\t22.29\t25.31\t44.37\t23.61\t28.51\t54.13\t24.29\t30.98\n",
            "\n",
            "Best results\n",
            "Metric\tHR@5\tMRR@5\tNDCG@5\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n",
            "Value\t34.71\t22.42\t25.42\t44.46\t23.76\t28.65\t54.47\t24.46\t31.18\n",
            "Epoch\t13\t10\t11\t10\t10\t10\t10\t10\t10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(float,\n",
              "            {'HR@10': 0.4446456760361727,\n",
              "             'HR@20': 0.5446836109105594,\n",
              "             'HR@5': 0.3470918635812969,\n",
              "             'MRR@10': 0.2376238990018782,\n",
              "             'MRR@20': 0.24458357172697717,\n",
              "             'MRR@5': 0.22424222223340234,\n",
              "             'NDCG@10': 0.2864745995767284,\n",
              "             'NDCG@20': 0.311788803589986,\n",
              "             'NDCG@5': 0.25419454403383623})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}
